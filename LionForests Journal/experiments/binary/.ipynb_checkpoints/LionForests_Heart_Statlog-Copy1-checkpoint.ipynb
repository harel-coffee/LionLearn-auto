{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the experiments on Heart Statlog dataset with LionForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "cpath = !pwd\n",
    "sys.path.append('C:\\\\Users\\\\iamollas\\\\Downloads\\\\LionForests Journal\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionforests import LionForests\n",
    "from datasets.dataset import Dataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the dataset and we set the feature and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = Dataset()\n",
    "X, y, feature_names, class_names = heart.load_heart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "iterr = 100\n",
    "import time  #10 trees\n",
    "for i in [1, 5, 7, 10]:\n",
    "    for j in [10, 100, 500]:\n",
    "        for o in ['sqrt','log2',0.75,None]:\n",
    "            parameters = [{\n",
    "                'max_depth': [i],\n",
    "                'max_features': [o],\n",
    "                'bootstrap': [False],\n",
    "                'min_samples_leaf' : [5],\n",
    "                'n_estimators': [j]\n",
    "            }]\n",
    "            lf = LionForests(class_names=class_names)\n",
    "            scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "            lf.train(X, y, scaler, feature_names, parameters)\n",
    "            tlf = 0\n",
    "            fr = 0\n",
    "            pr = 0\n",
    "            for inde in range(iterr):\n",
    "                t_a = time.time()\n",
    "                a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], False, None, None, True)\n",
    "                fr = fr + c - e\n",
    "                pr = pr + b - d\n",
    "                tlf = tlf + time.time() - t_a\n",
    "            results.append([str(i),str(j),str(o),'NoRed','_','_', tlf/iterr, fr/iterr, pr/iterr])\n",
    "            for k in ['1','2','3','12','13','23','123']:\n",
    "                    if '1' in k and '2' in k:\n",
    "                        for ara in ['apriori','fpgrowth']:\n",
    "                            for cla in ['kmedoids','OPTICS','SC']:\n",
    "                                tlf = 0\n",
    "                                fr = 0\n",
    "                                pr = 0\n",
    "                                for inde in range(iterr):\n",
    "                                    t_a = time.time()\n",
    "                                    a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], True, ara, cla, method=k)\n",
    "                                    fr = fr + c - e\n",
    "                                    pr = pr + b - d\n",
    "                                    tlf = tlf + time.time() - t_a\n",
    "                                results.append([str(i),str(j),str(o),k,ara,cla, tlf/iterr, fr/iterr, pr/iterr])\n",
    "                    elif '1' in k:\n",
    "                        for ara in ['apriori','fpgrowth']:\n",
    "                            tlf = 0\n",
    "                            fr = 0\n",
    "                            pr = 0\n",
    "                            for inde in range(iterr):\n",
    "                                t_a = time.time()\n",
    "                                a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], True, ara, None, method=k)\n",
    "                                fr = fr + c - e\n",
    "                                pr = pr + b - d\n",
    "                                tlf = tlf + time.time() - t_a\n",
    "                            results.append([str(i),str(j),str(o),k,ara,'_', tlf/iterr, fr/iterr, pr/iterr])\n",
    "                    elif '2' in k:\n",
    "                        for cla in ['kmedoids','OPTICS','SC']: \n",
    "                            tlf = 0\n",
    "                            fr = 0\n",
    "                            pr = 0\n",
    "                            for inde in range(iterr):\n",
    "                                t_a = time.time()\n",
    "                                a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], True, None, cla, method=k)\n",
    "                                fr = fr + c - e\n",
    "                                pr = pr + b - d\n",
    "                                tlf = tlf + time.time() - t_a\n",
    "                            results.append([str(i),str(j),str(o),k,'_',cla, tlf/iterr, fr/iterr, pr/iterr])\n",
    "                    else:\n",
    "                            tlf = 0\n",
    "                            fr = 0\n",
    "                            pr = 0\n",
    "                            for inde in range(iterr):\n",
    "                                t_a = time.time()\n",
    "                                a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], True, None, None, method=k)\n",
    "                                fr = fr + c - e\n",
    "                                pr = pr + b - d\n",
    "                                tlf = tlf + time.time() - t_a\n",
    "                            results.append([str(i),str(j),str(o),k,'_','_', tlf/iterr, fr/iterr, pr/iterr])\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.5s finished\n"
     ]
    }
   ],
   "source": [
    "parameters = [{\n",
    "    'max_depth': [5],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [False],\n",
    "    'min_samples_leaf' : [5],\n",
    "    'n_estimators': [100]\n",
    "}]\n",
    "lf = LionForests(class_names=class_names)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "lf.train(X, y, scaler, feature_names, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[88, 13, 67, 11]"
      ],
      "text/plain": [
       "[88, 13, 67, 11]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inde = 3\n",
    "lf.following_breadcrumbs(X[inde], True,method='13')[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n",
      "Warning! Error occured on kMedoids computation of this instance.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "iterr = 100\n",
    "import time  #10 trees\n",
    "for i in [1]:\n",
    "    for j in [10, 100]:\n",
    "        parameters = [{\n",
    "            'max_depth': [i],\n",
    "            'max_features': ['sqrt'],\n",
    "            'bootstrap': [False],\n",
    "            'min_samples_leaf' : [5],\n",
    "            'n_estimators': [j]\n",
    "        }]\n",
    "        lf = LionForests(class_names=class_names)\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        lf.train(X, y, scaler, feature_names, parameters)\n",
    "        tlf = 0\n",
    "        fr = 0\n",
    "        pr = 0\n",
    "        for inde in range(iterr):\n",
    "            t_a = time.time()\n",
    "            a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], False)\n",
    "            fr = fr + c - e\n",
    "            pr = pr + b - d\n",
    "            tlf = tlf + time.time() - t_a\n",
    "        results.append([\"MD_\"+str(i)+\"_NE_\"+str(j)+'_NoRed', tlf/iterr, fr/iterr, pr/iterr])\n",
    "        for k in ['1','2','3','12','13','23','123']:\n",
    "                if '1' in k and '2' in k:\n",
    "                    for ara in ['apriori','fpgrowth']:\n",
    "                        for cla in ['kmedoids','OPTICS','SC']:\n",
    "                            tlf = 0\n",
    "                            fr = 0\n",
    "                            pr = 0\n",
    "                            for inde in range(iterr):\n",
    "                                t_a = time.time()\n",
    "                                a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], True, ara, cla, method=k)\n",
    "                                fr = fr + c - e\n",
    "                                pr = pr + b - d\n",
    "                                tlf = tlf + time.time() - t_a\n",
    "                            name = \"MD_\"+str(i)+\"_NE_\"+str(j)+\"_M_\"+k+\"_AR_\"+ara+\"_CL_\"+cla\n",
    "                            results.append([name, tlf/iterr, fr/iterr, pr/iterr])\n",
    "                elif '1' in k:\n",
    "                    for ara in ['apriori','fpgrowth']:\n",
    "                        tlf = 0\n",
    "                        fr = 0\n",
    "                        pr = 0\n",
    "                        for inde in range(iterr):\n",
    "                            t_a = time.time()\n",
    "                            a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], True, ara, None, method=k)\n",
    "                            fr = fr + c - e\n",
    "                            pr = pr + b - d\n",
    "                            tlf = tlf + time.time() - t_a\n",
    "                        name = \"MD_\"+str(i)+\"_NE_\"+str(j)+\"_M_\"+k+\"_AR_\"+ara\n",
    "                        results.append([name, tlf/iterr, fr/iterr, pr/iterr])\n",
    "                elif '2' in k:\n",
    "                    for cla in ['kmedoids','OPTICS','SC']: \n",
    "                        tlf = 0\n",
    "                        fr = 0\n",
    "                        pr = 0\n",
    "                        for inde in range(iterr):\n",
    "                            t_a = time.time()\n",
    "                            a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], True, None, cla, method=k)\n",
    "                            fr = fr + c - e\n",
    "                            pr = pr + b - d\n",
    "                            tlf = tlf + time.time() - t_a\n",
    "                        name = \"MD_\"+str(i)+\"_NE_\"+str(j)+\"_M_\"+k+\"_CL_\"+cla\n",
    "                        results.append([name, tlf/iterr, fr/iterr, pr/iterr])\n",
    "                else:\n",
    "                        tlf = 0\n",
    "                        fr = 0\n",
    "                        pr = 0\n",
    "                        for inde in range(iterr):\n",
    "                            t_a = time.time()\n",
    "                            a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], True, None, None, method=k)\n",
    "                            fr = fr + c - e\n",
    "                            pr = pr + b - d\n",
    "                            tlf = tlf + time.time() - t_a\n",
    "                        name = \"MD_\"+str(i)+\"_NE_\"+str(j)+\"_M_\"+k\n",
    "                        results.append([name, tlf/iterr, fr/iterr, pr/iterr])\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MD_1_NE_10_NoRed', 0.21491077661514282, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_1_AR_apriori', 0.2187747550010681, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_1_AR_fpgrowth', 0.2183467650413513, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_2_CL_kmedoids', 0.21506417274475098, 0.96, 0.96]\n",
      "['MD_1_NE_10_M_2_CL_OPTICS', 0.2167215919494629, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_2_CL_SC', 0.23419156312942505, 0.86, 0.86]\n",
      "['MD_1_NE_10_M_3', 0.21526629686355592, 0.44, 0.96]\n",
      "['MD_1_NE_10_M_12_AR_apriori_CL_kmedoids', 0.21989779710769652, 0.96, 0.96]\n",
      "['MD_1_NE_10_M_12_AR_apriori_CL_OPTICS', 0.22165233850479127, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_12_AR_apriori_CL_SC', 0.23782361507415772, 0.86, 0.86]\n",
      "['MD_1_NE_10_M_12_AR_fpgrowth_CL_kmedoids', 0.21886398077011107, 0.96, 0.96]\n",
      "['MD_1_NE_10_M_12_AR_fpgrowth_CL_OPTICS', 0.22035908222198486, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_12_AR_fpgrowth_CL_SC', 0.2371159553527832, 0.86, 0.86]\n",
      "['MD_1_NE_10_M_13_AR_apriori', 0.22066093921661378, 0.64, 0.96]\n",
      "['MD_1_NE_10_M_13_AR_fpgrowth', 0.22362763404846192, 0.44, 0.96]\n",
      "['MD_1_NE_10_M_23_CL_kmedoids', 0.22042973279953004, 0.96, 0.96]\n",
      "['MD_1_NE_10_M_23_CL_OPTICS', 0.22317795038223268, 0.36, 0.96]\n",
      "['MD_1_NE_10_M_23_CL_SC', 0.23889763593673707, 0.86, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_apriori_CL_kmedoids', 0.2252155900001526, 0.96, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_apriori_CL_OPTICS', 0.22698607444763183, 0.29, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_apriori_CL_SC', 0.24445044994354248, 0.96, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_fpgrowth_CL_kmedoids', 0.22431655883789062, 0.96, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_fpgrowth_CL_OPTICS', 0.2261580228805542, 0.36, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_fpgrowth_CL_SC', 0.24317522287368776, 0.86, 0.96]\n",
      "['MD_1_NE_100_NoRed', 0.25387282609939577, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_1_AR_apriori', 0.25923622846603395, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_1_AR_fpgrowth', 0.2638933348655701, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_2_CL_kmedoids', 0.27268232583999635, 2.1, 8.83]\n",
      "['MD_1_NE_100_M_2_CL_OPTICS', 0.2914537692070007, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_2_CL_SC', 0.2993740963935852, 1.57, 9.37]\n",
      "['MD_1_NE_100_M_3', 0.2492280125617981, 0.23, 10.9]\n",
      "['MD_1_NE_100_M_12_AR_apriori_CL_kmedoids', 0.27311655044555666, 2.1, 8.83]\n",
      "['MD_1_NE_100_M_12_AR_apriori_CL_OPTICS', 0.28647297620773315, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_12_AR_apriori_CL_SC', 0.299803409576416, 1.57, 9.37]\n",
      "['MD_1_NE_100_M_12_AR_fpgrowth_CL_kmedoids', 0.2714210963249207, 2.1, 8.83]\n",
      "['MD_1_NE_100_M_12_AR_fpgrowth_CL_OPTICS', 0.29069221019744873, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_12_AR_fpgrowth_CL_SC', 0.3046107006072998, 1.57, 9.37]\n",
      "['MD_1_NE_100_M_13_AR_apriori', 0.2584911108016968, 0.2, 10.9]\n",
      "['MD_1_NE_100_M_13_AR_fpgrowth', 0.2552754831314087, 0.23, 10.9]\n",
      "['MD_1_NE_100_M_23_CL_kmedoids', 0.2657582426071167, 2.11, 11.38]\n",
      "['MD_1_NE_100_M_23_CL_OPTICS', 0.2823679065704346, 0.14, 10.9]\n",
      "['MD_1_NE_100_M_23_CL_SC', 0.289102349281311, 1.69, 11.33]\n",
      "['MD_1_NE_100_M_123_AR_apriori_CL_kmedoids', 0.26913067102432253, 2.1, 11.38]\n",
      "['MD_1_NE_100_M_123_AR_apriori_CL_OPTICS', 0.2843816041946411, 0.33, 10.9]\n",
      "['MD_1_NE_100_M_123_AR_apriori_CL_SC', 0.2929874110221863, 1.62, 11.33]\n",
      "['MD_1_NE_100_M_123_AR_fpgrowth_CL_kmedoids', 0.2656367111206055, 2.11, 11.38]\n",
      "['MD_1_NE_100_M_123_AR_fpgrowth_CL_OPTICS', 0.2829745674133301, 0.14, 10.9]\n",
      "['MD_1_NE_100_M_123_AR_fpgrowth_CL_SC', 0.29095510721206663, 1.69, 11.33]\n"
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MD_1_NE_10NoRed', 0.21512478351593017, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_1_AR_apriori', 0.21979347229003907, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_1_AR_fpgrowth', 0.21713338136672974, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_2_CL_kmedoids', 0.2160787582397461, 0.6, 1.02]\n",
      "['MD_1_NE_10_M_2_CL_OPTICS', 0.21732284784317016, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_2_CL_SC', 0.23349969387054442, 0.86, 0.86]\n",
      "['MD_1_NE_10_M_3', 0.21555511236190797, 0.44, 0.96]\n",
      "['MD_1_NE_10_M_12_AR_apriori_CL_kmedoids', 0.21999238014221192, 0.6, 1.02]\n",
      "['MD_1_NE_10_M_12_AR_apriori_CL_OPTICS', 0.22124712467193602, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_12_AR_apriori_CL_SC', 0.2373184633255005, 0.86, 0.86]\n",
      "['MD_1_NE_10_M_12_AR_fpgrowth_CL_kmedoids', 0.2211626172065735, 0.6, 1.02]\n",
      "['MD_1_NE_10_M_12_AR_fpgrowth_CL_OPTICS', 0.22054622888565065, 0.0, 0.0]\n",
      "['MD_1_NE_10_M_12_AR_fpgrowth_CL_SC', 0.23651870489120483, 0.86, 0.86]\n",
      "['MD_1_NE_10_M_13_AR_apriori', 0.21931565761566163, 0.64, 0.96]\n",
      "['MD_1_NE_10_M_13_AR_fpgrowth', 0.21774059534072876, 0.44, 0.96]\n",
      "['MD_1_NE_10_M_23_CL_kmedoids', 0.21493346929550172, 0.6, 1.02]\n",
      "['MD_1_NE_10_M_23_CL_OPTICS', 0.2165709614753723, 0.36, 0.96]\n",
      "['MD_1_NE_10_M_23_CL_SC', 0.23281531810760497, 0.86, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_apriori_CL_kmedoids', 0.21939430475234986, 0.6, 1.02]\n",
      "['MD_1_NE_10_M_123_AR_apriori_CL_OPTICS', 0.22018639087677003, 0.29, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_apriori_CL_SC', 0.23706443309783937, 0.96, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_fpgrowth_CL_kmedoids', 0.21866752624511718, 0.6, 1.02]\n",
      "['MD_1_NE_10_M_123_AR_fpgrowth_CL_OPTICS', 0.2204495096206665, 0.36, 0.96]\n",
      "['MD_1_NE_10_M_123_AR_fpgrowth_CL_SC', 0.2364520859718323, 0.86, 0.96]\n",
      "['MD_1_NE_100NoRed', 0.24786779880523682, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_1_AR_apriori', 0.25224517345428465, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_1_AR_fpgrowth', 0.2516627240180969, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_2_CL_kmedoids', 0.2614571452140808, 0.07, 4.93]\n",
      "['MD_1_NE_100_M_2_CL_OPTICS', 0.2785375642776489, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_2_CL_SC', 0.28676897764205933, 1.57, 9.37]\n",
      "['MD_1_NE_100_M_3', 0.24798936605453492, 0.23, 10.9]\n",
      "['MD_1_NE_100_M_12_AR_apriori_CL_kmedoids', 0.265914831161499, 0.07, 4.93]\n",
      "['MD_1_NE_100_M_12_AR_apriori_CL_OPTICS', 0.28277026653289794, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_12_AR_apriori_CL_SC', 0.2955848050117493, 1.57, 9.37]\n",
      "['MD_1_NE_100_M_12_AR_fpgrowth_CL_kmedoids', 0.27204291343688963, 0.07, 4.93]\n",
      "['MD_1_NE_100_M_12_AR_fpgrowth_CL_OPTICS', 0.28893577337265014, 0.0, 0.0]\n",
      "['MD_1_NE_100_M_12_AR_fpgrowth_CL_SC', 0.2967738938331604, 1.57, 9.37]\n",
      "['MD_1_NE_100_M_13_AR_apriori', 0.2576697182655334, 0.2, 10.9]\n",
      "['MD_1_NE_100_M_13_AR_fpgrowth', 0.2575823283195496, 0.23, 10.9]\n",
      "['MD_1_NE_100_M_23_CL_kmedoids', 0.26711569547653197, 0.24, 10.81]\n",
      "['MD_1_NE_100_M_23_CL_OPTICS', 0.2855629253387451, 0.14, 10.9]\n",
      "['MD_1_NE_100_M_23_CL_SC', 0.29305381059646607, 1.69, 11.33]\n",
      "['MD_1_NE_100_M_123_AR_apriori_CL_kmedoids', 0.27129035234451293, 0.24, 10.81]\n",
      "['MD_1_NE_100_M_123_AR_apriori_CL_OPTICS', 0.28855294942855836, 0.33, 10.9]\n",
      "['MD_1_NE_100_M_123_AR_apriori_CL_SC', 0.2946646356582642, 1.62, 11.33]\n",
      "['MD_1_NE_100_M_123_AR_fpgrowth_CL_kmedoids', 0.26578282833099365, 0.24, 10.81]\n",
      "['MD_1_NE_100_M_123_AR_fpgrowth_CL_OPTICS', 0.2823519825935364, 0.14, 10.9]\n",
      "['MD_1_NE_100_M_123_AR_fpgrowth_CL_SC', 0.2912070107460022, 1.69, 11.33]\n"
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the number of estimators and the best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.349790287017822\n"
     ]
    }
   ],
   "source": [
    "import time  #10 trees\n",
    "tlf = 0\n",
    "tlf2 = 0\n",
    "iterr=100\n",
    "for inde in range(100):\n",
    "    t_a = time.time()\n",
    "    a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], False, True, False, complexity=4)\n",
    "    tlf = tlf + time.time() - t_a\n",
    "print(tlf/iterr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.776662166118622\n",
      "8.894833698272706\n"
     ]
    }
   ],
   "source": [
    "import time  #10 trees\n",
    "tlf = 0\n",
    "tlf2 = 0\n",
    "iterr=100\n",
    "for inde in range(100):\n",
    "    t_a = time.time()\n",
    "    a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], False, True, False, complexity=4)\n",
    "    tlf = tlf + time.time() - t_a\n",
    "print(tlf/iterr)\n",
    "for inde in range(100):\n",
    "    t_a = time.time()\n",
    "    a2, b, c, d, e, f = lf2.following_breadcrumbs(X[inde], False, True, False, complexity=4)\n",
    "    tlf2 = tlf2 + time.time() - t_a\n",
    "print(tlf2/iterr)\n",
    "\n",
    "    #if a != a2:\n",
    "    #    print(inde,a,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21696030616760253\n",
      "3.288681070804596\n"
     ]
    }
   ],
   "source": [
    "import time  #10 trees\n",
    "tlf = 0\n",
    "tlf2 = 0\n",
    "iterr=100\n",
    "for inde in range(100):\n",
    "    t_a = time.time()\n",
    "    a, b, c, d, e, f = lf.following_breadcrumbs(X[inde], False, False, False, complexity=4)\n",
    "    tlf = tlf + time.time() - t_a\n",
    "print(tlf/iterr)\n",
    "for inde in range(100):\n",
    "    t_a = time.time()\n",
    "    a2, b, c, d, e, f = lf2.following_breadcrumbs(X[inde], False, False, False, complexity=4)\n",
    "    tlf2 = tlf2 + time.time() - t_a\n",
    "print(tlf2/iterr)\n",
    "\n",
    "    #if a != a2:\n",
    "    #    print(inde,a,§a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8188916011524707 , Number of estimators: 500\n",
      "RandomForestClassifier(bootstrap=False, max_depth=5, max_features='sqrt',\n",
      "                       min_samples_leaf=5, n_estimators=500, n_jobs=-1,\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "number_of_estimators = lf.model.n_estimators\n",
    "print(\"Accuracy:\",lf.accuracy,\", Number of estimators:\",lf.number_of_estimators)\n",
    "print(lf.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to produce explanations using lionForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "['if 6.5<=reversable defect<=7.0 & 3.5<=chest pain<=4.0 & 0.0<=number of major vessels<=0.5 & 1.55<=oldpeak<=1.7 & 0.5<=exercise induced angina<=1.0 & 128.005<=maximum heart rate achieved<=133.494 & 1.5<=the slope of the peak exercise<=2.5 & 184.999<=serum cholestoral<=199.496 & 119.0<=resting blood pressure<=121.491 then presence', 461, 13, 251, 9]"
      ],
      "text/plain": [
       "['if 6.5<=reversable defect<=7.0 & 3.5<=chest pain<=4.0 & 0.0<=number of major vessels<=0.5 & 1.55<=oldpeak<=1.7 & 0.5<=exercise induced angina<=1.0 & 128.005<=maximum heart rate achieved<=133.494 & 1.5<=the slope of the peak exercise<=2.5 & 184.999<=serum cholestoral<=199.496 & 119.0<=resting blood pressure<=121.491 then presence',\n",
       " 461,\n",
       " 13,\n",
       " 251,\n",
       " 9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.following_breadcrumbs(X[81], False, True, False, complexity=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the original explanation could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['if 6.5<=reversable defect<=7.0 & 3.5<=chest pain<=4.0 & 0.0<=number of major vessels<=0.5 & 1.55<=oldpeak<=1.7 & 0.5<=exercise induced angina<=1.0 & 128.005<=maximum heart rate achieved<=130.998 & 1.5<=the slope of the peak exercise<=2.5 & 29.002<=age<=41.497 & 184.999<=serum cholestoral<=199.496 & 0.5<=sex<=1.0 & 119.0<=resting blood pressure<=121.491 & 0.0<=resting electrocardiographic results<=0.5 & 0.0<=fasting blood sugar<=0.5 then presence', 461, 13, 461, 13]"
      ],
      "text/plain": [
       "['if 6.5<=reversable defect<=7.0 & 3.5<=chest pain<=4.0 & 0.0<=number of major vessels<=0.5 & 1.55<=oldpeak<=1.7 & 0.5<=exercise induced angina<=1.0 & 128.005<=maximum heart rate achieved<=130.998 & 1.5<=the slope of the peak exercise<=2.5 & 29.002<=age<=41.497 & 184.999<=serum cholestoral<=199.496 & 0.5<=sex<=1.0 & 119.0<=resting blood pressure<=121.491 & 0.0<=resting electrocardiographic results<=0.5 & 0.0<=fasting blood sugar<=0.5 then presence',\n",
       " 461,\n",
       " 13,\n",
       " 461,\n",
       " 13]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.following_breadcrumbs(X[81], False, False, False, complexity=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules, fpgrowth, fpmax\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "from utilities.kmedoids import kMedoids\n",
    "import shap\n",
    "import random\n",
    "from utilities.lionforests_utility import roundup, path_similarity, path_distance\n",
    "\n",
    "class LionForests2:\n",
    "    \"\"\"Class for interpreting random forests classifier through following_breacrumbs technique\"\"\"\n",
    "\n",
    "    def __init__(self, model=None, trained=False, utilizer=None, feature_names=None, class_names=None, categorical_features=None):\n",
    "        \"\"\"Init function\n",
    "        Args:\n",
    "            model: The trained RF model\n",
    "            utilizer: The preferred scaler\n",
    "            feature_names: The names of the features from our dataset\n",
    "            class_names: The names of the classes from our dataset\n",
    "        Attributes:\n",
    "            model: The classifier/ regression model\n",
    "            utilizer: The scaler, if any\n",
    "            trees: The trees of an trained ensemble system\n",
    "            feature_names: The names of the features\n",
    "            class_names: The names of the two classes\n",
    "            accuracy: The accuracy of the model (accuracy for classification, mse for regression):\n",
    "            min_max_feature_values: A helping dictionary for the path/feature reduction process\n",
    "            number_of_estimators: The amount of trees\n",
    "            ranked_features: The features ranked based on SHAP Values (Small-Medium Datasets) or Feature Importance (Huge Datasets)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.utilizer = utilizer\n",
    "        self.trees = None\n",
    "        if model is not None:\n",
    "            self.trees = model.estimators_\n",
    "        self.feature_names = feature_names\n",
    "        self.categorical_features = categorical_features\n",
    "        self.class_names = class_names\n",
    "        self.accuracy = 0\n",
    "        self.min_max_feature_values = {}\n",
    "        self.number_of_estimators = 0\n",
    "        self.ranked_features = {}\n",
    "        self.quorum = 0\n",
    "        if trained:\n",
    "            self._trained()\n",
    "        \n",
    "    def _trained(self):\n",
    "        self.trees = self.model.estimators_\n",
    "        self.number_of_estimators = self.model.n_estimators\n",
    "        self.quorum = int(self.number_of_estimators / 2 + 1)\n",
    "        \n",
    "    def transform_categorical_data(self, train_data, train_target, feature_names):\n",
    "        self.onehot_data = to_onehot_data\n",
    "        self.onehot_transformation = True\n",
    "        self.numerical_data = [i for i in feature_names if i not in to_onehot_data]\n",
    "        \n",
    "        \n",
    "        numerical = pd.DataFrame(train_data, columns=feature_names)[self.numerical_data].values\n",
    "        num = pd.DataFrame(numerical, columns=self.numerical_data)\n",
    "        if self.onehot_transformation:\n",
    "            onehot_encoded = pd.DataFrame(train_data, columns=feature_names)[to_onehot_data].values\n",
    "            enc = OneHotEncoder(handle_unknown='ignore')\n",
    "            enc.fit(onehot_encoded)\n",
    "            self.onehot_dictionary = {}\n",
    "            for i in range(len(to_onehot_data)):\n",
    "                self.onehot_dictionary[str('x' + str(i))] = to_onehot_data[i]\n",
    "            self.onehot_features = enc.get_feature_names()\n",
    "            one = pd.DataFrame(enc.transform(onehot_encoded).A, columns=self.onehot_features)\n",
    "\n",
    "        if self.ordinal_transformation:\n",
    "            ordinal_encoded = pd.DataFrame(train_data, columns=feature_names)[to_ordinal_data].values\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(ordinal_encoded)\n",
    "            self.ordinal_features = to_ordinal_data\n",
    "            ore = pd.DataFrame(enc.transform(ordinal_encoded), columns=to_ordinal_data)\n",
    "\n",
    "        if self.onehot_transformation and self.ordinal_transformation:\n",
    "            all_features = []\n",
    "            for f in self.onehot_features:\n",
    "                all_features.append(f)\n",
    "            for f in self.ordinal_features:\n",
    "                all_features.append(f)\n",
    "            for f in self.numerical_data:\n",
    "                all_features.append(f)\n",
    "            return pd.concat([one, ore, num], axis=1), train_target, all_features\n",
    "        elif self.onehot_transformation:\n",
    "            all_features = []\n",
    "            for f in self.onehot_features:\n",
    "                all_features.append(f)\n",
    "            for f in self.numerical_data:\n",
    "                all_features.append(f)\n",
    "            return pd.concat([one, num], axis=1), train_target, all_features\n",
    "        elif self.ordinal_transformation:\n",
    "            all_features = []\n",
    "            for f in self.ordinal_features:\n",
    "                all_features.append(f)\n",
    "            for f in self.numerical_data:\n",
    "                all_features.append(f)\n",
    "            return pd.concat([ore, num], axis=1), train_target, all_features\n",
    "\n",
    "\n",
    "    def train(self, train_data, train_target, scaling_method=None, feature_names=None, params=None, categorical_features=None):\n",
    "        \"\"\" train function is used to train an RF model and extract information like accuracy, model, trees and\n",
    "        min_max_feature_values among all trees\n",
    "        Args:\n",
    "            train_data: The data we are going to use to train the random forest\n",
    "            train_target: The targets of our train data\n",
    "            scaling_method: The preffered scaling method. The deafult is MinMaxScaler with feature range -1 to 1\n",
    "            feature_names: The names of the features from our dataset\n",
    "            params: The parameters for our gridSearchCV to select the best RF model\n",
    "        \"\"\"\n",
    "        self.categorical_features = categorical_features\n",
    "        if feature_names is not None:\n",
    "            self.feature_names = feature_names\n",
    "        if scaling_method is not None:  # load scaling\n",
    "            self.utilizer = scaling_method\n",
    "        else:\n",
    "            self.utilizer = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.utilizer.fit(train_data)\n",
    "        train_data = self.utilizer.transform(train_data)\n",
    "        random_forest = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "        parameters = params\n",
    "        if parameters is None:\n",
    "            parameters = [{\n",
    "                'max_depth': [5],#[1, 5, 7, 10],#1, 5, 7, 10\n",
    "                'max_features': ['sqrt'],#['sqrt', 'log2', 0.75, None], #'sqrt', 'log2', 0.75, None\n",
    "                'bootstrap': [True],#[True, False], #True, False\n",
    "                'min_samples_leaf' : [2],#[1, 2, 5, 10, 0.10], #1, 2, 5, 10, 0.10\n",
    "                'n_estimators': [250]#[10, 100, 500, 1000] #10, 100, 500, 1000\n",
    "            }]\n",
    "        clf = GridSearchCV(estimator=random_forest, param_grid=parameters, cv=10, n_jobs=-1, verbose=1, scoring='f1')\n",
    "        clf.fit(train_data, train_target)\n",
    "        self.accuracy = clf.best_score_\n",
    "        self.model = clf.best_estimator_\n",
    "        self.trees = self.model.estimators_\n",
    "        self.number_of_estimators = self.model.n_estimators\n",
    "        self.quorum = int(self.number_of_estimators / 2 + 1)\n",
    "        for i in range(len(self.feature_names)):\n",
    "            self.min_max_feature_values[self.feature_names[i]] = [min(train_data[:, i]), max(train_data[:, i])]\n",
    "        for ind in range(len(self.class_names)):\n",
    "            d = {'Feature': feature_names, 'Importance': self.model.feature_importances_}\n",
    "            self.ranked_features[self.class_names[ind]] = \\\n",
    "                pd.DataFrame(data=d).sort_values(by=['Importance'], ascending=False)['Feature'].values\n",
    "\n",
    "\n",
    "    def path_finder(self, instance, info=False):\n",
    "        \"\"\"path_finder function finds\n",
    "        Args:\n",
    "            instance: The instance we want to find the paths\n",
    "            info: If we want to show information about the features in the paths\n",
    "        Return:\n",
    "            a list which contains a dictionary with features as keys and their min max ranges as values, as well as the\n",
    "            number of the paths\n",
    "        \"\"\"\n",
    "        if self.utilizer is not None:\n",
    "            instance = self.utilizer.transform([instance])[0]\n",
    "        prediction = int(self.model.predict([instance])[0])\n",
    "        total_leq = {}  # All the rules with less equal operators ex: a <= 1\n",
    "        total_b = {}  # All the rules with bigger than operators ex: c > 0.1\n",
    "        rules = []\n",
    "        ranges = []\n",
    "        for tree in self.trees:\n",
    "            tree_prediction = int(tree.predict([instance])[0])\n",
    "            if tree_prediction == prediction:\n",
    "                path = tree.decision_path([instance])\n",
    "                rule = 'if '\n",
    "                leq = {}  # leq: less equal ex: x <= 1\n",
    "                b = {}  # b: bigger ex: x > 0.6\n",
    "                local_range = {}\n",
    "                for node in path.indices:\n",
    "                    feature_id = tree.tree_.feature[node]\n",
    "                    feature = self.feature_names[feature_id]\n",
    "                    threshold = tree.tree_.threshold[node]\n",
    "                    if threshold != -2.0:\n",
    "                        if instance[feature_id] <= threshold:\n",
    "                            leq.setdefault(feature, []).append(threshold)\n",
    "                        else:\n",
    "                            b.setdefault(feature, []).append(threshold)\n",
    "                for k in leq:\n",
    "                    rule = rule + k + \"<=\" + str(min(leq[k])) + \" and \"\n",
    "                    total_leq.setdefault(k, []).append(min(leq[k]))  # !!\n",
    "                    local_range.setdefault(k, []).append(['<=', min(leq[k])])  # !!\n",
    "                for k in b:\n",
    "                    rule = rule + k + \">\" + str(max(b[k])) + \" and \"\n",
    "                    total_b.setdefault(k, []).append(max(b[k]))  # !!\n",
    "                    local_range.setdefault(k, []).append(['>', max(b[k])])  # !!\n",
    "                rule = rule[:-4] + \"then \" + str(self.class_names[int(tree.predict([instance])[0])])\n",
    "                rules.append(rule)\n",
    "                ranges.append(local_range)\n",
    "\n",
    "        if info:\n",
    "            print(\"Number of paths:\", len(rules))\n",
    "            for k in total_leq:\n",
    "                print(\n",
    "                    \"{:<10} {:<2} {:<7} | {:<7}\".format(k[:10], '<=', roundup(min(total_leq[k]), 4), len(total_leq[k])))\n",
    "            for k in total_b:\n",
    "                print(\"{:<10} {:<2} {:<7} | {:<7}\".format(k[:10], '>', roundup(max(total_b[k]), 4), len(total_b[k])))\n",
    "        del instance, prediction, total_leq, total_b\n",
    "        return [ranges, len(rules)]  # If i want rules at natural language i have to add rules variable here too\n",
    "\n",
    "    def following_breadcrumbs(self, instance, info=False, reduction=True, save_plots=False, complexity=4, instance_quorum=0, medoids=0):\n",
    "        \"\"\"following_breadcrumbs function finds a single range rule which will be the explanation for the prediction\n",
    "        of this instance\n",
    "        Args:\n",
    "            instance: The instance we want to find the paths\n",
    "            info: If we want to show information about the features in the paths\n",
    "            reduction: The targets of our train data\n",
    "            save_plots: The bar and stacked area plots for every feature will be saved\n",
    "            complexity:\n",
    "        Return:\n",
    "            a feature range rule which will be the explanation\n",
    "        \"\"\"\n",
    "\n",
    "        if instance_quorum <= 0:\n",
    "            instance_quorum = self.quorum\n",
    "\n",
    "        number_of_medoids = medoids\n",
    "        if medoids <= 0:\n",
    "            number_of_medoids = 5\n",
    "            if self.number_of_estimators < 5:\n",
    "                number_of_medoids = self.number_of_estimators\n",
    "            if self.number_of_estimators >= 100:\n",
    "                number_of_medoids = int(math.ceil(instance_quorum * 3 / 22))  # 1100 = 11 * 100\n",
    "\n",
    "        rules = self.path_finder(instance, info)[0]\n",
    "        original_number_of_rules = len(rules)\n",
    "\n",
    "        items = set()\n",
    "        for pr in rules:\n",
    "            for p in pr:\n",
    "                items.add(p)\n",
    "        local_feature_names = list(items)\n",
    "        original_number_of_features = len(local_feature_names)\n",
    "        if reduction:\n",
    "            temp_rules = self.reduce_rules(rules, instance_quorum, number_of_medoids)\n",
    "            if len(temp_rules[0]) != 0:\n",
    "                rules = temp_rules[0]\n",
    "                local_feature_names = temp_rules[1]\n",
    "        rule = \"if \"\n",
    "        temp_f_mins = {}\n",
    "        temp_f_maxs = {}\n",
    "        feature_rule_limits = {}\n",
    "        for feature in self.feature_names:\n",
    "            if feature in local_feature_names:\n",
    "                bars, bars_len = self._pre_feature_range_caluclation(rules,feature,complexity)\n",
    "                if bars != False:\n",
    "                    aggregation = self._aggregated_feature_range(bars, feature, save_plots, complexity)\n",
    "                    temp_f_mins[feature] = aggregation[0]\n",
    "                    temp_f_maxs[feature] = aggregation[1]\n",
    "        f_mins = []\n",
    "        f_maxs = []\n",
    "        for feature in self.feature_names:\n",
    "            if feature in temp_f_mins:\n",
    "                f_mins.append(temp_f_mins[feature])\n",
    "            else:\n",
    "                f_mins.append(0)\n",
    "            if feature in temp_f_maxs:\n",
    "                f_maxs.append(temp_f_maxs[feature])\n",
    "            else:\n",
    "                f_maxs.append(0)\n",
    "        if self.utilizer is not None:\n",
    "            instance = self.utilizer.transform([instance])[0]\n",
    "        class_name = self.class_names[self.model.predict([instance])[0]]\n",
    "        if self.categorical_features is not None:\n",
    "            for ranked_f in self.ranked_features[class_name]:\n",
    "                f = self.feature_names.index(ranked_f)\n",
    "                it_was_categ = False\n",
    "                for ff in self.categorical_features:\n",
    "                    if ff in self.feature_names[f]:\n",
    "                        if self.feature_names[f] in local_feature_names:\n",
    "                            if self.utilizer is not None:\n",
    "                                mmi = self.utilizer.inverse_transform(np.array([f_mins, f_mins]))[0][f]\n",
    "                                mma = self.utilizer.inverse_transform(np.array([f_maxs, f_maxs]))[0][f]\n",
    "                            else:\n",
    "                                mmi = np.array([f_mins, f_mins])[0][f]\n",
    "                                mma = np.array([f_maxs, f_maxs])[0][f]\n",
    "                            \n",
    "                            if str(round(mma, 3)) == '1.0':\n",
    "                                feature_rule_limits[self.feature_names[f]] = [mmi, mma]\n",
    "                                rule = rule + self.feature_names[f] + \" & \"\n",
    "                            it_was_categ = True\n",
    "                if not it_was_categ:\n",
    "                    if self.feature_names[f] in local_feature_names:\n",
    "                        if self.utilizer is not None:\n",
    "                            mmi = self.utilizer.inverse_transform(np.array([f_mins, f_mins]))[0][f]\n",
    "                            mma = self.utilizer.inverse_transform(np.array([f_maxs, f_maxs]))[0][f]\n",
    "                        else:\n",
    "                            mmi = np.array([f_mins, f_mins])[0][f]\n",
    "                            mma = np.array([f_maxs, f_maxs])[0][f]\n",
    "                        feature_rule_limits[self.feature_names[f]] = [mmi, mma]\n",
    "                        rule = rule + str(round(mmi, 3)) + \"<=\" + self.feature_names[f] + \"<=\" + str(round(mma, 3)) + \" & \"\n",
    "        else:\n",
    "            for ranked_f in self.ranked_features[class_name]:\n",
    "                f = self.feature_names.index(ranked_f)\n",
    "                if self.feature_names[f] in local_feature_names:\n",
    "                    #if self.utilizer is not None:\n",
    "                    #    mmi = self.utilizer.inverse_transform(np.array([f_mins, f_mins]))[0][f]\n",
    "                    #    mma = self.utilizer.inverse_transform(np.array([f_maxs, f_maxs]))[0][f]\n",
    "                    #else:\n",
    "                    mmi = np.array([f_mins, f_mins])[0][f]\n",
    "                    mma = np.array([f_maxs, f_maxs])[0][f] #ena tab mesa\n",
    "                    feature_rule_limits[self.feature_names[f]] = [mmi, mma]\n",
    "                    rule = rule + str(round(mmi, 3)) + \"<=\" + self.feature_names[f] + \"<=\" + str(round(mma, 3)) + \" & \"\n",
    "        del temp_f_maxs, temp_f_mins, f_maxs, f_mins, feature_rule_limits\n",
    "        return [rule[:-3] + \" then \" + class_name,original_number_of_rules, original_number_of_features, len(rules), len(local_feature_names), rules]\n",
    "\n",
    "    def reduce_rules(self, rules, instance_quorum, number_of_medoids):\n",
    "        \"\"\"following_breadcrumbs function finds\n",
    "        Args:\n",
    "            instance: The instance we want to find the paths\n",
    "            reduction: The targets of our train data\n",
    "            save_plots: The bar and stacked area plots for every feature will be saved\n",
    "        Return:\n",
    "\n",
    "        \"\"\"\n",
    "        get_itemsets = []\n",
    "        for pr in rules:\n",
    "            itemset = []\n",
    "            for p in pr:\n",
    "                itemset.append(p)\n",
    "            get_itemsets.append(itemset)\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(get_itemsets).transform(get_itemsets)\n",
    "        df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        #apriori, fpgrowth, fpmax\n",
    "        frequent_itemsets = association_rules(apriori(df, min_support=0.1, use_colnames=True), metric=\"support\",\n",
    "                                              min_threshold=0.1).sort_values(by=\"confidence\", ascending=True)\n",
    "        size = 0\n",
    "        k = 1\n",
    "        size_of_ar = len(list(list(frequent_itemsets['antecedents'])))\n",
    "        items = set()\n",
    "        reduced_rules = rules\n",
    "        new_feature_list = []\n",
    "        for pr in reduced_rules:\n",
    "            for p in pr:\n",
    "                items.add(p)\n",
    "            new_feature_list = list(items)\n",
    "        while size < instance_quorum and k < size_of_ar:\n",
    "            feature_set = set()\n",
    "            for i in range(0, k):\n",
    "                for j in list(list(frequent_itemsets['antecedents'])[i]):\n",
    "                    feature_set.add(j)\n",
    "            new_feature_list = list(feature_set)\n",
    "            redundant_features = [i for i in self.feature_names if i not in new_feature_list]\n",
    "            reduced_rules = []\n",
    "            for i in rules:\n",
    "                if sum([1 for j in redundant_features if j in i]) == 0:\n",
    "                    reduced_rules.append(i)\n",
    "            size = len(reduced_rules)\n",
    "            k += 1\n",
    "        del get_itemsets, te, te_ary, df, frequent_itemsets\n",
    "        if len(reduced_rules) < instance_quorum:\n",
    "            reduced_rules = rules\n",
    "            for pr in reduced_rules:\n",
    "                for p in pr:\n",
    "                    items.add(p)\n",
    "                new_feature_list = list(items)\n",
    "        if len(reduced_rules) > instance_quorum: #If we need more reduction on path level\n",
    "            A = []\n",
    "            for k in range(len(reduced_rules)):\n",
    "                B = []\n",
    "                for j in range(len(reduced_rules)):\n",
    "                    if k == j:\n",
    "                        B.append(0)  # or 1?\n",
    "                    else:\n",
    "                        sim = path_similarity(reduced_rules[k], reduced_rules[j], new_feature_list,\n",
    "                                              self.min_max_feature_values)\n",
    "                        #sim = path_distance(reduced_rules[k], reduced_rules[j], new_feature_list,\n",
    "                                              #self.min_max_feature_values) #Tested with distance metric of iForest\n",
    "                        B.append(1 - sim)\n",
    "                A.append(B)\n",
    "            A = np.array(A)\n",
    "            MS, S = kMedoids(A, number_of_medoids)\n",
    "            medoids_sorted = sorted(S, key=lambda k: len(S[k]), reverse=True)\n",
    "            k = 0\n",
    "            size = 0\n",
    "            reduced_rules_medoids = []\n",
    "            while size < instance_quorum and k < len(medoids_sorted):\n",
    "                for j in S[medoids_sorted[k]]:\n",
    "                    reduced_rules_medoids.append(reduced_rules[j])\n",
    "                k += 1\n",
    "                size = len(reduced_rules_medoids)\n",
    "            items = set()\n",
    "            if len(reduced_rules_medoids) >= instance_quorum:\n",
    "                reduced_rules = reduced_rules_medoids\n",
    "                for pr in reduced_rules_medoids:\n",
    "                    for p in pr:\n",
    "                        items.add(p)\n",
    "                new_feature_list = list(items)\n",
    "        if len(reduced_rules) > instance_quorum:\n",
    "            random.shuffle(reduced_rules)\n",
    "            reduced_rules = reduced_rules[:instance_quorum]\n",
    "            items = set()\n",
    "            for pr in reduced_rules:\n",
    "                for p in pr:\n",
    "                    items.add(p)\n",
    "            new_feature_list = list(items)\n",
    "        return [reduced_rules, new_feature_list]\n",
    "\n",
    "    def _pre_feature_range_caluclation(self, rules, feature, complexity=4):\n",
    "        mi = self.min_max_feature_values[feature][0]\n",
    "        ma = self.min_max_feature_values[feature][1]\n",
    "        for i in rules:\n",
    "            if feature in i:\n",
    "                if len(i[feature]) == 1:\n",
    "                    if i[feature][0][0] == \"<=\":\n",
    "                        if ma < i[feature][0][1]:\n",
    "                            ma = i[feature][0][1]\n",
    "                    else:\n",
    "                        if mi > i[feature][0][1]:\n",
    "                            mi = i[feature][0][1]\n",
    "                else:\n",
    "                    if mi > i[feature][1][1]:\n",
    "                        mi = i[feature][1][1]\n",
    "                    if ma < i[feature][0][1]:\n",
    "                        ma = i[feature][0][1]\n",
    "\n",
    "        bars = []\n",
    "        temp_count = 0\n",
    "        for i in rules:\n",
    "            if feature in i:\n",
    "                temp_count += 1\n",
    "                if len(i[feature]) == 1:\n",
    "                    if i[feature][0][0] == \"<=\":\n",
    "                        bars.append(np.arange(roundup(mi, complexity), roundup(i[feature][0][1], complexity),\n",
    "                                              (10 ** (-complexity))))\n",
    "                    else:\n",
    "                        bars.append(np.arange(roundup(i[feature][0][1], complexity), roundup(ma, complexity),\n",
    "                                              (10 ** (-complexity))))\n",
    "                else:\n",
    "                    mm = [roundup(i[feature][0][1], complexity), roundup(i[feature][1][1], complexity)]\n",
    "                    bars.append(np.arange(min(mm), max(mm), (10 ** (-complexity))))\n",
    "        if temp_count == 0:\n",
    "            return False, False\n",
    "        bars_len = [len(bar) for bar in bars]\n",
    "        return bars, bars_len\n",
    "\n",
    "    def _aggregated_feature_range(self, bars, feature, save_plots=False, complexity=4):\n",
    "        \"\"\"_aggregated_feature_range function returns the min and max value from the intersection of all paths\n",
    "        Args:\n",
    "            feature: the feature which range we want to find\n",
    "            save_plots: if yes then it will save the bar and stacked area plots of each feature\n",
    "            complexity: determines how many digits we will use to descritize\n",
    "        Return:\n",
    "            min max of the intersect area of all paths for a feature\n",
    "        \"\"\"\n",
    "        mi = self.min_max_feature_values[feature][0]\n",
    "        ma = self.min_max_feature_values[feature][1]\n",
    "\n",
    "        if save_plots:\n",
    "            plt.figure(figsize=(16, 10))\n",
    "            plt.title(feature)\n",
    "            plt.ylabel('No of Rules')\n",
    "            plt.xlabel('Value')\n",
    "            for i in range(len(bars)):\n",
    "                plt.plot(bars[i], len(bars[i]) * [i + 1], linewidth=8)\n",
    "            plt.savefig(feature + \"BarsPlot.png\")\n",
    "\n",
    "        temp_bars = []\n",
    "        for i in bars:\n",
    "            bar = set()\n",
    "            for j in i:\n",
    "                bar.add(int((roundup(j, complexity) - roundup(mi, complexity)) * (10 ** complexity)))\n",
    "            temp_bars.append(bar)\n",
    "        bars = temp_bars\n",
    "        del temp_bars\n",
    "\n",
    "        st = {}\n",
    "        for i in bars:\n",
    "            for j in i:\n",
    "                if not int(j) in st:\n",
    "                    st[int(j)] = 1\n",
    "                else:\n",
    "                    st[int(j)] += 1\n",
    "        del bars\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        max_v = -1\n",
    "        for key, value in st.items():\n",
    "            if max_v < value:\n",
    "                max_v = value\n",
    "            x.append((key + int(roundup(mi, complexity) * (10 ** complexity))) / (10 ** complexity))\n",
    "            y.append(value)\n",
    "        x, y = zip(*sorted(zip(x, y)))\n",
    "        x_2 = []\n",
    "        x_3 = []\n",
    "        y_2 = []\n",
    "        for key, value in st.items():\n",
    "            x_2.append((key + int(roundup(mi, complexity) * (10 ** complexity))) / (10 ** complexity))\n",
    "            if max_v == value:\n",
    "                x_3.append((key + int(roundup(mi, complexity) * (10 ** complexity))) / (10 ** complexity))\n",
    "                y_2.append(value)\n",
    "            else:\n",
    "                y_2.append(0)\n",
    "        del st\n",
    "\n",
    "        if save_plots:\n",
    "            x_2, y_2 = zip(*sorted(zip(x_2, y_2)))\n",
    "            plt.figure(figsize=(16, 10))\n",
    "            plt.title(feature)\n",
    "            plt.ylabel('No of Rules')\n",
    "            plt.xlabel('Value')\n",
    "            plt.stackplot(x, y)\n",
    "            plt.stackplot(x, y_2, colors='c')\n",
    "            plt.savefig(feature + \"StackedAreaPlot.png\")\n",
    "        del x, y, x_2, y_2\n",
    "        return [min(x_3), max(x_3)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionets",
   "language": "python",
   "name": "lionets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
