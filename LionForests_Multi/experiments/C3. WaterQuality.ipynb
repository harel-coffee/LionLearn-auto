{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14d5d35",
   "metadata": {},
   "source": [
    "## LionForests Multi Label Experiments: Water Quality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4839bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "cpath = !pwd\n",
    "sys.path.append('/usr/src/app/algorithms/')\n",
    "sys.path.append('/usr/src/app/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09530df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets.dataset import Dataset\n",
    "import pandas as pd\n",
    "from utilities.dummy_utilizer import DummyUtilizer\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from algorithms.MARLENA.marlena.marlena.marlena import MARLENA\n",
    "from algorithms.simpleSurrogate import GlobalSurrogateTree, LocalSurrogateTree\n",
    "from algorithms.CHIRPS.structures import data_container\n",
    "import algorithms.CHIRPS.routines as rt\n",
    "import algorithms.CHIRPS.structures as strcts\n",
    "from scipy import sparse\n",
    "from algorithms.anchor.anchor_tabular import AnchorTabularExplainer\n",
    "\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d377210",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c8e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_quality = Dataset()\n",
    "X, y, feature_names, label_names = water_quality.load_water_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282b564",
   "metadata": {},
   "source": [
    "Initialize LionForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf03b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionforestsmulti import LionForests\n",
    "lf = LionForests(None, False, None, feature_names, label_names)#,'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b949349",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{\n",
    "    'max_depth': [10],\n",
    "    'max_features': [None],\n",
    "    'bootstrap': [True],\n",
    "    'min_samples_leaf' : [1],\n",
    "    'n_estimators': [100]\n",
    "}]\n",
    "lf.fit(X, list(y), parameters, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098f2b7",
   "metadata": {},
   "source": [
    "Best RF model for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16895505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, max_features=None, n_jobs=-1,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835fef3e",
   "metadata": {},
   "source": [
    "Example explanation for option \"all predicted labelsets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7170b7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 3.224<=std_temp<=3.321 & 23.121<=std_pH<=23.572 & 2.699<=conduct<=2.783 & 3.625<=o2<=3.666 & 4.124<=o2sat<=4.186 & 0.622<=co2<=0.758 & 2.936<=hardness<=2.955 & 0.432<=no2<=0.489 & 2.976<=no3<=3.574 & 0.095<=nh4<=0.099 & 0.263<=po4<=0.326 & 0.771<=cl<=0.782 & 3.171<=sio2<=3.188 & 0.716<=kmno4<=0.885 & 0.703<=k2cr2o7<=0.717 & 0.243<=bod<=0.252 then 25400 29600 17300 34500 49700 50390 55800 59300 37880'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'all',True, method='1')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97e6cf",
   "metadata": {},
   "source": [
    "Example explanation for option \"per label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946aa6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 3.224<=std_temp<=3.321 & 23.121<=std_pH<=23.572 & 2.699<=conduct<=2.783 & 3.625<=o2<=3.666 & 4.124<=o2sat<=4.186 & 2.936<=hardness<=3.014 & 0.432<=no2<=0.489 & 2.976<=no3<=3.574 & 0.095<=nh4<=0.099 & 0.263<=po4<=0.326 & 0.771<=cl<=0.782 & 3.171<=sio2<=3.188 & 0.676<=kmno4<=0.885 & 0.703<=k2cr2o7<=0.72 & 0.243<=bod<=0.252 then 17300'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'per label',True, method='1')['17300'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a444c",
   "metadata": {},
   "source": [
    "Example explanation for option \"frequent pairs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8916b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 3.224<=std_temp<=3.321 & 23.121<=std_pH<=23.572 & 2.699<=conduct<=2.783 & 3.625<=o2<=3.666 & 4.124<=o2sat<=4.186 & 0.622<=co2<=0.758 & 2.936<=hardness<=2.955 & 0.432<=no2<=0.489 & 2.976<=no3<=3.574 & 0.095<=nh4<=0.099 & 0.263<=po4<=0.326 & 0.771<=cl<=0.782 & 3.171<=sio2<=3.188 & 0.716<=kmno4<=0.885 & 0.703<=k2cr2o7<=0.717 & 0.243<=bod<=0.252 then 29600 17300'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'frequent pairs',7, True, method='1')[('29600','17300')][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515e251",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c038d55",
   "metadata": {},
   "source": [
    "1. Comparison between LF variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c1c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': [None],\n",
    "        'bootstrap': [True],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [100]\n",
    "    }]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    lf = LionForests(None, False, scaler, feature_names, label_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        # LionForests\n",
    "        def lf_rule_all(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'all')[5]\n",
    "            rule = {}\n",
    "            for key, value in temp.items():\n",
    "                rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "            return rule\n",
    "\n",
    "        def lf_rule_per_label(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'per label')\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        def lf_rule_pairs(instance):\n",
    "            temp = lf.explain_n_wise(\n",
    "                instance, 'frequent pairs', len(label_names))\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        return {'lf-a': lf_rule_all, 'lf-l': lf_rule_per_label, 'lf-p': lf_rule_pairs}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions, test, feature_names, label_names, lf,\n",
    "                                'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-a': 0, 'lf-l': 0, 'lf-p': 0}\n",
    "    rule_length = {'lf-a': 0, 'lf-l': 0, 'lf-p': 0}\n",
    "    f_precision = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "    time_response = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "    rules = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "    for tesd_ind in range(len(test)):\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "            #print(tesd_ind,'sunexise madam')\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-a':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(y_test_temp[tesd_ind]), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp_lf)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                elif name == 'lf-l':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'lf-p':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        labeles = []\n",
    "                        for kk in key:\n",
    "                            labeles.append(list(label_names).index(kk))\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                temp_prediction = [y_test_temp[tesd_ind][j] for j in range(\n",
    "                                    len(y_test_temp[tesd_ind])) if j in labeles]\n",
    "                                temperatura = [y_test_temp[co][j] for j in range(\n",
    "                                    len(y_test_temp[co])) if j in labeles]\n",
    "                                precision.append(\n",
    "                                    [temp_prediction, temperatura])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, 0], precision[:, 1], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b99460",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    test_size.append(len(X_test)-results[-1])\n",
    "    total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbfed7a",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "858d78b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lf-a | 0.0096  0.000 | 16.5372 0.421 | 1.0000  0.000 | 0.3770  0.003\n",
      "lf-l | 0.0096  0.000 | 60.7630 6.220 | 1.0000  0.000 | 1.3374  0.122\n",
      "lf-p | 0.0096  0.000 | 153.3407 16.756 | 1.0000  0.000 | 2.8663  0.290\n"
     ]
    }
   ],
   "source": [
    "full_coverage = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "rule_length = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "f_precision ={'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "f_time = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec64007",
   "metadata": {},
   "source": [
    "2. Comparison between LF-a and multi-label explainability techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870bd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': [None],\n",
    "        'bootstrap': [True],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [100]\n",
    "    }]\n",
    "    lf = LionForests(None, False, None, feature_names, label_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        # LionForests\n",
    "        def lf_rule_all(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'all')[5]\n",
    "            rule = {}\n",
    "            for key, value in temp.items():\n",
    "                rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "            return rule\n",
    "\n",
    "        gt = GlobalSurrogateTree(\n",
    "            train, predictions, feature_names, task, random_state)\n",
    "        print('    GT Ready')\n",
    "        lt = LocalSurrogateTree(\n",
    "            train, predictions, feature_names, task, 150, random_state)\n",
    "\n",
    "        def marlena(instance):\n",
    "            m1 = MARLENA(neigh_type='mixed', random_state=42)\n",
    "            i2e = pd.Series(instance, index=feature_names)\n",
    "            X2E = pd.DataFrame(train, columns=feature_names)\n",
    "            rule, _, _, _, _ = m1.extract_explanation(i2e, X2E, lf.model, feature_names, [],\n",
    "                                                      label_names, k=10, size=50, alpha=0.7)\n",
    "\n",
    "            path = rule.split('->')[0][1:-2]\n",
    "            prediction = rule.split('->')[1][1:]\n",
    "            conjuctions = {}\n",
    "            if '{}' in rule:\n",
    "                print(rule)\n",
    "            else:\n",
    "                for conj in path.split('\\n'):\n",
    "                    temp_conj = conj.strip(',')\n",
    "                    if temp_conj[0] == ' ':\n",
    "                        temp_conj = temp_conj[1:]\n",
    "                    if '>' in temp_conj:\n",
    "                        temp = temp_conj.split(' > ')\n",
    "                        if temp[0] in conjuctions:\n",
    "                            position = 0\n",
    "                            if len(conjuctions[temp[0]]) == 2:\n",
    "                                if conjuctions[temp[0]][1][0] == '>':\n",
    "                                    position = 1\n",
    "                            if conjuctions[temp[0]][position][0] == '>':\n",
    "                                if conjuctions[temp[0]][position][1] > float(temp[1]):\n",
    "                                    conjuctions[temp[0]][position][1] = float(\n",
    "                                        temp[1])\n",
    "                            elif conjuctions[temp[0]][0][0] == '<=':\n",
    "                                conjuctions[temp[0]].append(\n",
    "                                    ['>', float(temp[1])])\n",
    "\n",
    "                        else:\n",
    "                            conjuctions[temp[0]] = [['>', float(temp[1])]]\n",
    "                    else:\n",
    "                        temp = temp_conj.split(' <= ')\n",
    "                        if temp[0] in conjuctions:\n",
    "                            position = 0\n",
    "                            if len(conjuctions[temp[0]]) == 2:\n",
    "                                if conjuctions[temp[0]][1][0] == '<=':\n",
    "                                    position = 1\n",
    "\n",
    "                            if conjuctions[temp[0]][position][0] == '<=':\n",
    "                                if conjuctions[temp[0]][position][1] <= float(temp[1]):\n",
    "                                    conjuctions[temp[0]][position][1] = float(\n",
    "                                        temp[1])\n",
    "                            elif conjuctions[temp[0]][0][0] == '>':\n",
    "                                conjuctions[temp[0]].append(\n",
    "                                    ['<=', float(temp[1])])\n",
    "                        else:\n",
    "                            conjuctions[temp[0]] = [['<=', float(temp[1])]]\n",
    "            local_prediction = np.zeros((len(label_names)), dtype=int)\n",
    "            for label in range(len(label_names)):\n",
    "                if label_names[label] in prediction:\n",
    "                    local_prediction[label] = 1\n",
    "            return (conjuctions, local_prediction)\n",
    "\n",
    "        return {'lf-a': lf_rule_all, 'gs': gt.rule, 'ls': lt.rule, 'ma': marlena}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions, test, feature_names, label_names, lf,\n",
    "                                'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-a': 0, 'ls': 0, 'gs': 0, 'ma': 0}\n",
    "    rule_length = {'lf-a': 0, 'ls': 0, 'gs': 0, 'ma': 0}\n",
    "    f_precision = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "    time_response = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "    rules = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        # or trees in lf.model.estimators_:\n",
    "        #   print(trees.predict_proba([x_test_temp_lf[tesd_ind]]))\n",
    "        # rint(y_test_temp[tesd_ind])\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "            #print(tesd_ind,'sunexise madam')\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-a':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(y_test_temp[tesd_ind]), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp_lf)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                elif name == 'ls' or name == 'gs' or name == 'ma':\n",
    "                    ts = time.time()\n",
    "                    rule, prediction = method(x_test_temp[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp:\n",
    "                        res = rule_cov(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(prediction), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c846d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10, random_state = 77)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    test_size.append(len(X_test)-results[-1])\n",
    "    total_results.append(results)\n",
    "    folds = folds + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b12c7",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2459fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lf-a | 0.0095  0.000 | 16.3655 0.404 | 1.0000  0.000 | 0.9377  0.042\n",
      "gs | 0.0193  0.004 | 8.7795 0.365 | 0.6645  0.048 | 0.0004  0.000\n",
      "ls | 0.0294  0.004 | 6.6755 0.207 | 0.6545  0.026 | 1.4057  0.034\n",
      "ma | 0.0473  0.009 | 5.9790 0.335 | 0.6242  0.050 | 2.0390  0.011\n"
     ]
    }
   ],
   "source": [
    "full_coverage = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "rule_length = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "f_precision ={'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "f_time = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d05e9e",
   "metadata": {},
   "source": [
    "3. Comparison between LF-l and \"per label\" explainability techniques (CHIRPS/ANCHORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b356a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': [None],\n",
    "        'bootstrap': [True],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [100]\n",
    "    }]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    lf = LionForests(None, False, scaler, feature_names, label_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        def lf_rule_per_label(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'per label')\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        inddd = 0\n",
    "        anchors = {}\n",
    "        for label_name in label_names:\n",
    "            explainer = AnchorTabularExplainer(\n",
    "                ['not '+label_name, label_name], feature_names, train)\n",
    "            anchors[inddd] = explainer\n",
    "            inddd += 1\n",
    "\n",
    "        def anchors_method(instance):\n",
    "            prediction = lf.model.predict([instance])[0]\n",
    "            rules = {}\n",
    "            for pred in range(len(prediction)):\n",
    "                if prediction[pred] == 1:\n",
    "                    explainer = anchors[pred]\n",
    "\n",
    "                    def my_model(x):\n",
    "                        return lf.model.predict(x)[:, pred]\n",
    "                    exp = explainer.explain_instance(\n",
    "                        instance, my_model, threshold=0.95)\n",
    "                    anchors_dict = {}\n",
    "                    for i in exp.names():\n",
    "                        terms = i.split(' ')\n",
    "                        if len(terms) == 3:\n",
    "                            anchors_dict[terms[0]] = [\n",
    "                                [terms[1], float(terms[2])]]\n",
    "                        else:\n",
    "                            anchors_dict[terms[2]] = [[terms[3], float(terms[4])], [\n",
    "                                terms[1], float(terms[0])]]\n",
    "                    rules[label_names[pred]] = anchors_dict\n",
    "            return rules\n",
    "\n",
    "        chts = time.time()\n",
    "        chirps = {}\n",
    "        inddd = 0\n",
    "        for label_name in label_names:\n",
    "            explainer = AnchorTabularExplainer(\n",
    "                ['not '+label_name, label_name], feature_names, train)\n",
    "            # CHIRPS =======================================================================================\n",
    "            project_dir = 'C:\\\\Users\\\\iamollas\\\\Downloads\\\\LionForests Journal\\\\algorithms\\\\CHIRPS'\n",
    "            temp_frame = pd.DataFrame(np.hstack((train, y_train[:, inddd].reshape(\n",
    "                len(y_train), 1))), columns=feature_names+['class'])\n",
    "            temp_frame['class'] = temp_frame['class'].astype(int)\n",
    "            #temp_frame = temp_frame.replace({\"class\": {1: 2}})\n",
    "            #temp_frame = temp_frame.replace({\"class\": {0: 1}})\n",
    "\n",
    "            mydata = data_container(\n",
    "                data=temp_frame, class_col='class', var_names=feature_names,\n",
    "                project_dir=project_dir, save_dir='WQ'+str(inddd), random_state=random_state)\n",
    "            meta_data = mydata.get_meta()\n",
    "            f_walker = strcts.classification_trees_walker(\n",
    "                forest=model, inddd=inddd, meta_data=meta_data)\n",
    "            f_walker.forest_walk(instances=test, labels=model.predict(test)[\n",
    "                                 :, inddd], forest_walk_async=False)\n",
    "\n",
    "            explanations = strcts.CHIRPS_container(f_walker.path_detail,\n",
    "                                                   forest=model,\n",
    "                                                   # any representative sample can be used\n",
    "                                                   sample_instances=sparse.csr_matrix(\n",
    "                                                       train),\n",
    "                                                   sample_labels=predictions[:, inddd],\n",
    "                                                   meta_data=meta_data)\n",
    "            explanations.run_explanations(target_classes=model.predict(test)[:, inddd],  # we're explaining the prediction, not the true label!\n",
    "                                          explanation_async=False,\n",
    "                                          random_state=random_state,\n",
    "                                          which_trees='majority',\n",
    "                                          alpha_paths=0.0,\n",
    "                                          support_paths=0.1,\n",
    "                                          score_func=1,\n",
    "                                          precis_threshold=0.8,\n",
    "                                          disc_path_bins=2,\n",
    "                                          merging_bootstraps=10,\n",
    "                                          pruning_bootstraps=10,\n",
    "                                          delta=0.2)\n",
    "            chirps[inddd] = explanations\n",
    "            inddd += 1\n",
    "        chte = (time.time() - chts)/len(test)\n",
    "\n",
    "        def chirps_method(instance, idx):\n",
    "            prediction = lf.model.predict([instance])[0]\n",
    "            rules = {}\n",
    "            for pred in range(len(prediction)):\n",
    "                if prediction[pred] == 1:\n",
    "                    chirps_dict = {}\n",
    "                    explanations = chirps[pred]\n",
    "                    for i in explanations.explainers[idx].pruned_rule:\n",
    "                        if i[1]:\n",
    "                            chirps_dict[i[0]] = [['<=', float(i[2])]]\n",
    "                        else:\n",
    "                            chirps_dict[i[0]] = [['>', float(i[2])]]\n",
    "                    rules[label_names[pred]] = chirps_dict\n",
    "            return rules, 0, chte\n",
    "\n",
    "        return {'lf-l': lf_rule_per_label, 'an': anchors_method, 'ch': chirps_method}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions,\n",
    "                                test, feature_names, label_names, lf, 'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-l': 0, 'an': 0, 'ch': 0}\n",
    "    rule_length = {'lf-l': 0, 'an': 0, 'ch': 0}\n",
    "    f_precision = {'lf-l': [], 'an': [], 'ch': []}\n",
    "    time_response = {'lf-l': [], 'an': [], 'ch': []}\n",
    "    rules = {'lf-l': [], 'an': [], 'ch': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        # or trees in lf.model.estimators_:\n",
    "        #   print(trees.predict_proba([x_test_temp_lf[tesd_ind]]))\n",
    "        # rint(y_test_temp[tesd_ind])\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-l':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'an':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp:\n",
    "                            res = rule_cov(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'ch':\n",
    "                    rule, op, te = method(x_test_temp[tesd_ind], tesd_ind)\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp:\n",
    "                            res = rule_cov(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c3bf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10, random_state=77)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    test_size.append(len(X_test))\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317efe1b",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e583ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lf-l | 0.0091  0.000 | 57.6415 5.714 | 1.0000  0.000 | 1.3705  0.112\n",
      "an | 0.0372  0.011 | 31.0481 3.474 | 0.9628  0.023 | 302.0667  44.434\n",
      "ch | 0.3754  0.046 | 10.3594 1.366 | 0.7583  0.046 | 8.2391  0.500\n"
     ]
    }
   ],
   "source": [
    "full_coverage = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "rule_length = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_precision = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_time = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
