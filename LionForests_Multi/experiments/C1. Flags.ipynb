{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbbc02d8",
   "metadata": {},
   "source": [
    "## LionForests Multi Label Experiments: Flags Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f098b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "cpath = !pwd\n",
    "sys.path.append('/usr/src/app/algorithms/')\n",
    "sys.path.append('/usr/src/app/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c991815",
   "metadata": {},
   "source": [
    "Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09530df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import Dataset\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from utilities.dummy_utilizer import DummyUtilizer\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from algorithms.MARLENA.marlena.marlena.marlena import MARLENA\n",
    "from algorithms.simpleSurrogate import GlobalSurrogateTree, LocalSurrogateTree\n",
    "from algorithms.CHIRPS.structures import data_container\n",
    "import algorithms.CHIRPS.routines as rt\n",
    "import algorithms.CHIRPS.structures as strcts\n",
    "from scipy import sparse\n",
    "from algorithms.anchor.anchor_tabular import AnchorTabularExplainer\n",
    "import numpy as np\n",
    "import random\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb027d0",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129acb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Dataset()\n",
    "X, y, feature_names, label_names = flags.load_flags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463f9c3",
   "metadata": {},
   "source": [
    "Initialize LionForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78064772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionforestsmulti import LionForests\n",
    "lf = LionForests(None, False, None, list(feature_names), label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b949349",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{\n",
    "    'max_depth': [10],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [False],\n",
    "    'min_samples_leaf': [5],\n",
    "    'n_estimators': [500]\n",
    "}]\n",
    "lf.fit(X, y, parameters, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7096b3",
   "metadata": {},
   "source": [
    "Best RF model for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afdc9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=10, max_features='sqrt',\n",
       "                       min_samples_leaf=5, n_estimators=500, n_jobs=-1,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2489b0f",
   "metadata": {},
   "source": [
    "Example explanation for option \"all predicted labelsets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a41ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 3.5<=landmass<=4.5 & 1.0<=zone<=1.5 & 1832.0<=area<=2582.5 & 19.0<=population<=21.0 & 7.5<=language<=8.5 & 1.5<=religion<=2.5 & 1.5<=bars<=2.5 & 0.0<=stripes<=0.5 & 2.5<=colours<=3.5 & 0.0<=circles<=0.5 & 0.0<=crosses<=0.5 & 0.0<=saltires<=0.5 & 0.0<=quarters<=0.5 & 0.5<=sunstars<=1.5 & 0.5<=crescent<=1.0 & 0.0<=triangle<=0.5 & 0.0<=icon<=0.5 & 0.0<=animate<=0.5 & 0.0<=text<=0.5 then red green white'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'all',True, method='1')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a2668",
   "metadata": {},
   "source": [
    "Example explanation for option \"per label\" for label \"red\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de57736c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 3.5<=landmass<=4.5 & 1.0<=zone<=1.5 & 1832.0<=area<=2582.5 & 19.0<=population<=21.0 & 7.5<=language<=8.5 & 1.5<=religion<=2.5 & 1.5<=bars<=2.5 & 0.0<=stripes<=0.5 & 2.5<=colours<=3.5 & 0.0<=circles<=0.5 & 0.0<=crosses<=0.5 & 0.0<=saltires<=0.5 & 0.0<=quarters<=0.5 & 0.5<=sunstars<=1.5 & 0.5<=crescent<=1.0 & 0.0<=triangle<=0.5 & 0.0<=icon<=0.5 & 0.0<=animate<=0.5 & 0.0<=text<=0.5 then red'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'per label',True, method='1')['red'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934fe2e",
   "metadata": {},
   "source": [
    "Example explanation for option \"frequent pairs\" for labelset \"red green white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df59b3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 3.5<=landmass<=4.5 & 1.0<=zone<=1.5 & 1832.0<=area<=2582.5 & 19.0<=population<=21.0 & 7.5<=language<=8.5 & 1.5<=religion<=2.5 & 1.5<=bars<=2.5 & 0.0<=stripes<=0.5 & 2.5<=colours<=3.5 & 0.0<=circles<=0.5 & 0.0<=crosses<=0.5 & 0.0<=saltires<=0.5 & 0.0<=quarters<=0.5 & 0.5<=sunstars<=1.5 & 0.5<=crescent<=1.0 & 0.0<=triangle<=0.5 & 0.0<=icon<=0.5 & 0.0<=animate<=0.5 & 0.0<=text<=0.5 then red green white'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'frequent pairs',7, True, method='1')[('red', 'green', 'white')][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d09130",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599869b",
   "metadata": {},
   "source": [
    "1. Comparison between LF variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8c1c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': ['sqrt'],\n",
    "        'bootstrap': [False],\n",
    "        'min_samples_leaf': [5],\n",
    "        'n_estimators': [500]\n",
    "    }]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    lf = LionForests(None, False, scaler, feature_names, label_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        # LionForests\n",
    "        def lf_rule_all(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'all')[5]\n",
    "            rule = {}\n",
    "            for key, value in temp.items():\n",
    "                rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "            return rule\n",
    "\n",
    "        def lf_rule_per_label(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'per label')\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        def lf_rule_pairs(instance):\n",
    "            temp = lf.explain_n_wise(\n",
    "                instance, 'frequent pairs', len(label_names))\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        return {'lf-a': lf_rule_all, 'lf-l': lf_rule_per_label, 'lf-p': lf_rule_pairs}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions,\n",
    "                                test, feature_names, label_names, lf, 'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-a': 0, 'lf-l': 0, 'lf-p': 0}\n",
    "    rule_length = {'lf-a': 0, 'lf-l': 0, 'lf-p': 0}\n",
    "    f_precision = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "    time_response = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "    rules = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        for name, method in rule_generator.items():\n",
    "            if name == 'lf-a':\n",
    "                ts = time.time()\n",
    "                rule = method(x_test_temp_lf[tesd_ind])\n",
    "                te = time.time() - ts\n",
    "                coverage = 0\n",
    "                precision = []\n",
    "                co = 0\n",
    "                for i in x_test_temp_lf:\n",
    "                    res = rule_cov_LF(i, feature_names, rule)\n",
    "                    coverage = coverage + res\n",
    "                    if res == 1:\n",
    "                        precision.append(\n",
    "                            [list(y_test_temp[tesd_ind]), list(y_test_temp[co])])\n",
    "                    co = co + 1\n",
    "                if len(precision) >= 1:\n",
    "                    precision = np.array(precision)\n",
    "                    f_precision[name].append(precision_score(\n",
    "                        precision[:, 0], precision[:, 1], average='micro'))\n",
    "                full_coverage[name] = full_coverage[name] + \\\n",
    "                    coverage/len(x_test_temp_lf)\n",
    "                rules[name].append(rule)\n",
    "                len_rule = len(rule)\n",
    "            elif name == 'lf-l':\n",
    "                ts = time.time()\n",
    "                rule = method(x_test_temp_lf[tesd_ind])\n",
    "                te = time.time() - ts\n",
    "                total_coverage = 0\n",
    "                total_precision = []\n",
    "                len_rule = 0\n",
    "                for key in rule.keys():\n",
    "                    temp_rule = rule[key]\n",
    "                    len_rule += len(temp_rule)\n",
    "                    label_index = list(label_names).index(key)\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        total_precision.append(precision_score(\n",
    "                            precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                    total_coverage += coverage/len(x_test_temp_lf)\n",
    "                total_precision = [k for k in total_precision if str(\n",
    "                    k) != str(np.average([]))]\n",
    "                f_precision[name].append(np.array(total_precision).mean())\n",
    "                full_coverage[name] = full_coverage[name] + \\\n",
    "                    total_coverage/len(rule.keys())\n",
    "                rules[name].append(rule)\n",
    "            elif name == 'lf-p':\n",
    "                ts = time.time()\n",
    "                rule = method(x_test_temp_lf[tesd_ind])\n",
    "                te = time.time() - ts\n",
    "                total_coverage = 0\n",
    "                total_precision = []\n",
    "                len_rule = 0\n",
    "                for key in rule.keys():\n",
    "                    temp_rule = rule[key]\n",
    "                    len_rule += len(temp_rule)\n",
    "                    labeles = []\n",
    "                    for kk in key:\n",
    "                        labeles.append(list(label_names).index(kk))\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            temp_prediction = [y_test_temp[tesd_ind][j] for j in range(\n",
    "                                len(y_test_temp[tesd_ind])) if j in labeles]\n",
    "                            temperatura = [y_test_temp[co][j] for j in range(\n",
    "                                len(y_test_temp[co])) if j in labeles]\n",
    "                            precision.append([temp_prediction, temperatura])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        total_precision.append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    total_coverage += coverage/len(x_test_temp_lf)\n",
    "                total_precision = [k for k in total_precision if str(\n",
    "                    k) != str(np.average([]))]\n",
    "                f_precision[name].append(np.array(total_precision).mean())\n",
    "                full_coverage[name] = full_coverage[name] + \\\n",
    "                    total_coverage/len(rule.keys())\n",
    "                rules[name].append(rule)\n",
    "\n",
    "            time_response[name].append(te)\n",
    "            rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b99460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    test_size.append(len(X_test))\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))\n",
    "    results = measure(X_train, X_test, y_train, y_test,\n",
    "                      feature_names, label_names)\n",
    "    total_results.append(results)\n",
    "    folds = folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a118c86",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51367ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lf-a | 0.0479  0.002 | 18.2350 0.487 | 1.0000  0.000 | 1.0778  0.244\n",
      "lf-l | 0.0478  0.002 | 62.5750 5.235 | 1.0000  0.000 | 5.6585  0.397\n",
      "lf-p | 0.0478  0.002 | 111.7150 7.315 | 1.0000  0.000 | 7.3839  0.483\n"
     ]
    }
   ],
   "source": [
    "full_coverage = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "rule_length = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "f_precision = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "f_time = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([]))]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f'\n",
    "          % (np.array(full_coverage[name]).mean(), np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(), np.array(\n",
    "                 rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(), np.array(\n",
    "                 f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(), np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4da7e7",
   "metadata": {},
   "source": [
    "2. Comparison between LF-a and multi-label explainability techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bbd95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': ['sqrt'],\n",
    "        'bootstrap': [False],\n",
    "        'min_samples_leaf': [5],\n",
    "        'n_estimators': [500]\n",
    "    }]\n",
    "    lf = LionForests(None, False, None, feature_names, label_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        # LionForests\n",
    "        def lf_rule_all(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'all')[5]\n",
    "            rule = {}\n",
    "            for key, value in temp.items():\n",
    "                rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "            return rule\n",
    "\n",
    "        gt = GlobalSurrogateTree(\n",
    "            train, predictions, feature_names, task, random_state)\n",
    "        print('    GT Ready')\n",
    "        lt = LocalSurrogateTree(\n",
    "            train, predictions, feature_names, task, 150, random_state)\n",
    "\n",
    "        def marlena(instance):\n",
    "            m1 = MARLENA(neigh_type='mixed', random_state=42)\n",
    "            i2e = pd.Series(instance, index=feature_names)\n",
    "            X2E = pd.DataFrame(train, columns=feature_names)\n",
    "            rule, _, _, _, _ = m1.extract_explanation(i2e, X2E, lf.model, feature_names, [],\n",
    "                                                      label_names, k=10, size=50, alpha=0.7)\n",
    "\n",
    "            path = rule.split('->')[0][1:-2]\n",
    "            prediction = rule.split('->')[1][1:]\n",
    "            conjuctions = {}\n",
    "            if '{}' in rule:\n",
    "                print(rule)\n",
    "            else:\n",
    "                for conj in path.split('\\n'):\n",
    "                    temp_conj = conj.strip(',')\n",
    "                    if temp_conj[0] == ' ':\n",
    "                        temp_conj = temp_conj[1:]\n",
    "                    if '>' in temp_conj:\n",
    "                        temp = temp_conj.split(' > ')\n",
    "                        if temp[0] in conjuctions:\n",
    "                            position = 0\n",
    "                            if len(conjuctions[temp[0]]) == 2:\n",
    "                                if conjuctions[temp[0]][1][0] == '>':\n",
    "                                    position = 1\n",
    "                            if conjuctions[temp[0]][position][0] == '>':\n",
    "                                if conjuctions[temp[0]][position][1] > float(temp[1]):\n",
    "                                    conjuctions[temp[0]][position][1] = float(\n",
    "                                        temp[1])\n",
    "                            elif conjuctions[temp[0]][0][0] == '<=':\n",
    "                                conjuctions[temp[0]].append(\n",
    "                                    ['>', float(temp[1])])\n",
    "\n",
    "                        else:\n",
    "                            conjuctions[temp[0]] = [['>', float(temp[1])]]\n",
    "                    else:\n",
    "                        temp = temp_conj.split(' <= ')\n",
    "                        if temp[0] in conjuctions:\n",
    "                            position = 0\n",
    "                            if len(conjuctions[temp[0]]) == 2:\n",
    "                                if conjuctions[temp[0]][1][0] == '<=':\n",
    "                                    position = 1\n",
    "\n",
    "                            if conjuctions[temp[0]][position][0] == '<=':\n",
    "                                if conjuctions[temp[0]][position][1] <= float(temp[1]):\n",
    "                                    conjuctions[temp[0]][position][1] = float(\n",
    "                                        temp[1])\n",
    "                            elif conjuctions[temp[0]][0][0] == '>':\n",
    "                                conjuctions[temp[0]].append(\n",
    "                                    ['<=', float(temp[1])])\n",
    "                        else:\n",
    "                            conjuctions[temp[0]] = [['<=', float(temp[1])]]\n",
    "            local_prediction = np.zeros((len(label_names)), dtype=int)\n",
    "            for label in range(len(label_names)):\n",
    "                if label_names[label] in prediction:\n",
    "                    local_prediction[label] = 1\n",
    "            return (conjuctions, local_prediction)\n",
    "\n",
    "        return {'lf-a': lf_rule_all, 'gs': gt.rule, 'ls': lt.rule, 'ma': marlena}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions, test, feature_names, label_names, lf,\n",
    "                                'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-a': 0, 'ls': 0, 'gs': 0, 'ma': 0}\n",
    "    rule_length = {'lf-a': 0, 'ls': 0, 'gs': 0, 'ma': 0}\n",
    "    f_precision = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "    time_response = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "    rules = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        # or trees in lf.model.estimators_:\n",
    "        #   print(trees.predict_proba([x_test_temp_lf[tesd_ind]]))\n",
    "        # rint(y_test_temp[tesd_ind])\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "            #print(tesd_ind,'sunexise madam')\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-a':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(y_test_temp[tesd_ind]), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp_lf)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                elif name == 'ls' or name == 'gs' or name == 'ma':\n",
    "                    ts = time.time()\n",
    "                    rule, prediction = method(x_test_temp[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp:\n",
    "                        res = rule_cov(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(prediction), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0942b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    test_size.append(len(X_test))\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ada8ec",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3ed798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lf-a | 0.0479  0.002 | 18.2400 0.485 | 1.0000  0.000 | 1.0456  0.103\n",
      "gs | 0.0786  0.014 | 5.2250 0.650 | 0.8964  0.047 | 0.0004  0.000\n",
      "ls | 0.0823  0.018 | 5.3100 0.617 | 0.9078  0.027 | 3.4054  0.055\n",
      "ma | 0.1263  0.032 | 4.4150 0.201 | 0.8496  0.045 | 0.8634  0.004\n"
     ]
    }
   ],
   "source": [
    "full_coverage = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "rule_length = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "f_precision ={'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "f_time = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24a97a",
   "metadata": {},
   "source": [
    "3. Comparison between LF-l and \"per label\" explainability techniques (CHIRPS/ANCHORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c7db988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': ['sqrt'],\n",
    "        'bootstrap': [False],\n",
    "        'min_samples_leaf': [5],\n",
    "        'n_estimators': [500]\n",
    "    }]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    lf = LionForests(None, False, scaler, feature_names, label_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        def lf_rule_per_label(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'per label')\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        inddd = 0\n",
    "        anchors = {}\n",
    "        for label_name in label_names:\n",
    "            explainer = AnchorTabularExplainer(\n",
    "                ['not '+label_name, label_name], feature_names, train)\n",
    "            anchors[inddd] = explainer\n",
    "            inddd += 1\n",
    "\n",
    "        def anchors_method(instance):\n",
    "            prediction = lf.model.predict([instance])[0]\n",
    "            rules = {}\n",
    "            for pred in range(len(prediction)):\n",
    "                if prediction[pred] == 1:\n",
    "                    explainer = anchors[pred]\n",
    "\n",
    "                    def my_model(x):\n",
    "                        return lf.model.predict(x)[:, pred]\n",
    "                    exp = explainer.explain_instance(\n",
    "                        instance, my_model, threshold=0.95)\n",
    "                    anchors_dict = {}\n",
    "                    for i in exp.names():\n",
    "                        terms = i.split(' ')\n",
    "                        if len(terms) == 3:\n",
    "                            anchors_dict[terms[0]] = [\n",
    "                                [terms[1], float(terms[2])]]\n",
    "                        else:\n",
    "                            anchors_dict[terms[2]] = [[terms[3], float(terms[4])], [\n",
    "                                terms[1], float(terms[0])]]\n",
    "                    rules[label_names[pred]] = anchors_dict\n",
    "            return rules\n",
    "\n",
    "        chts = time.time()\n",
    "        chirps = {}\n",
    "        inddd = 0\n",
    "        for label_name in label_names:\n",
    "            explainer = AnchorTabularExplainer(\n",
    "                ['not '+label_name, label_name], feature_names, train)\n",
    "            # CHIRPS =======================================================================================\n",
    "            project_dir = 'C:\\\\Users\\\\iamollas\\\\Downloads\\\\LionForests Journal\\\\algorithms\\\\CHIRPS'\n",
    "            temp_frame = pd.DataFrame(np.hstack((train, y_train[:, inddd].reshape(\n",
    "                len(y_train), 1))), columns=feature_names+['class'])\n",
    "            temp_frame['class'] = temp_frame['class'].astype(int)\n",
    "            #temp_frame = temp_frame.replace({\"class\": {1: 2}})\n",
    "            #temp_frame = temp_frame.replace({\"class\": {0: 1}})\n",
    "\n",
    "            mydata = data_container(\n",
    "                data=temp_frame, class_col='class', var_names=feature_names,\n",
    "                project_dir=project_dir, save_dir='flags'+str(inddd), random_state=random_state)\n",
    "            meta_data = mydata.get_meta()\n",
    "            f_walker = strcts.classification_trees_walker(\n",
    "                forest=model, inddd=inddd, meta_data=meta_data)\n",
    "            f_walker.forest_walk(instances=test, labels=model.predict(test)[\n",
    "                                 :, inddd], forest_walk_async=False)\n",
    "\n",
    "            explanations = strcts.CHIRPS_container(f_walker.path_detail,\n",
    "                                                   forest=model,\n",
    "                                                   # any representative sample can be used\n",
    "                                                   sample_instances=sparse.csr_matrix(\n",
    "                                                       train),\n",
    "                                                   sample_labels=predictions[:, inddd],\n",
    "                                                   meta_data=meta_data)\n",
    "            explanations.run_explanations(target_classes=model.predict(test)[:, inddd],  # we're explaining the prediction, not the true label!\n",
    "                                          explanation_async=False,\n",
    "                                          random_state=random_state,\n",
    "                                          which_trees='majority',\n",
    "                                          alpha_paths=0.0,\n",
    "                                          support_paths=0.1,\n",
    "                                          score_func=1,\n",
    "                                          precis_threshold=0.9,\n",
    "                                          disc_path_bins=4,\n",
    "                                          merging_bootstraps=20,\n",
    "                                          pruning_bootstraps=20,\n",
    "                                          delta=0.2,\n",
    "                                          weighting='kldiv')\n",
    "            chirps[inddd] = explanations\n",
    "            inddd += 1\n",
    "        chte = (time.time() - chts)/len(test)\n",
    "\n",
    "        def chirps_method(instance, idx):\n",
    "            prediction = lf.model.predict([instance])[0]\n",
    "            rules = {}\n",
    "            for pred in range(len(prediction)):\n",
    "                if prediction[pred] == 1:\n",
    "                    chirps_dict = {}\n",
    "                    explanations = chirps[pred]\n",
    "                    for i in explanations.explainers[idx].pruned_rule:\n",
    "                        if i[1]:\n",
    "                            chirps_dict[i[0]] = [['<=', float(i[2])]]\n",
    "                        else:\n",
    "                            chirps_dict[i[0]] = [['>', float(i[2])]]\n",
    "                    rules[label_names[pred]] = chirps_dict\n",
    "            return rules, 0, chte\n",
    "\n",
    "        return {'lf-l': lf_rule_per_label, 'an': anchors_method, 'ch': chirps_method}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions,\n",
    "                                test, feature_names, label_names, lf, 'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-l': 0, 'an': 0, 'ch': 0}\n",
    "    rule_length = {'lf-l': 0, 'an': 0, 'ch': 0}\n",
    "    f_precision = {'lf-l': [], 'an': [], 'ch': []}\n",
    "    time_response = {'lf-l': [], 'an': [], 'ch': []}\n",
    "    rules = {'lf-l': [], 'an': [], 'ch': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        for name, method in rule_generator.items():\n",
    "            if name == 'lf-l':\n",
    "                ts = time.time()\n",
    "                rule = method(x_test_temp_lf[tesd_ind])\n",
    "                te = time.time() - ts\n",
    "                total_coverage = 0\n",
    "                total_precision = []\n",
    "                len_rule = 0\n",
    "                for key in rule.keys():\n",
    "                    temp_rule = rule[key]\n",
    "                    len_rule += len(temp_rule)\n",
    "                    label_index = list(label_names).index(key)\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        total_precision.append(precision_score(\n",
    "                            precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                    total_coverage += coverage/len(x_test_temp_lf)\n",
    "                total_precision = [k for k in total_precision if str(\n",
    "                    k) != str(np.average([]))]\n",
    "                f_precision[name].append(np.array(total_precision).mean())\n",
    "                full_coverage[name] = full_coverage[name] + \\\n",
    "                    total_coverage/len(rule.keys())\n",
    "                rules[name].append(rule)\n",
    "            elif name == 'an':\n",
    "                ts = time.time()\n",
    "                rule = method(x_test_temp[tesd_ind])\n",
    "                te = time.time() - ts\n",
    "                total_coverage = 0\n",
    "                total_precision = []\n",
    "                len_rule = 0\n",
    "                for key in rule.keys():\n",
    "                    temp_rule = rule[key]\n",
    "                    len_rule += len(temp_rule)\n",
    "                    label_index = list(label_names).index(key)\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp:\n",
    "                        res = rule_cov(i, feature_names, temp_rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        total_precision.append(precision_score(\n",
    "                            precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                    total_coverage += coverage/len(x_test_temp)\n",
    "                total_precision = [k for k in total_precision if str(\n",
    "                    k) != str(np.average([]))]\n",
    "                f_precision[name].append(np.array(total_precision).mean())\n",
    "                full_coverage[name] = full_coverage[name] + \\\n",
    "                    total_coverage/len(rule.keys())\n",
    "                rules[name].append(rule)\n",
    "            elif name == 'ch':\n",
    "                rule, op, te = method(x_test_temp[tesd_ind], tesd_ind)\n",
    "                total_coverage = 0\n",
    "                total_precision = []\n",
    "                len_rule = 0\n",
    "                for key in rule.keys():\n",
    "                    temp_rule = rule[key]\n",
    "                    len_rule += len(temp_rule)\n",
    "                    label_index = list(label_names).index(key)\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp:\n",
    "                        res = rule_cov(i, feature_names, temp_rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        total_precision.append(precision_score(\n",
    "                            precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                    total_coverage += coverage/len(x_test_temp)\n",
    "                total_precision = [k for k in total_precision if str(\n",
    "                    k) != str(np.average([]))]\n",
    "                f_precision[name].append(np.array(total_precision).mean())\n",
    "                full_coverage[name] = full_coverage[name] + \\\n",
    "                    total_coverage/len(rule.keys())\n",
    "                rules[name].append(rule)\n",
    "\n",
    "            time_response[name].append(te)\n",
    "            rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e0a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    test_size.append(len(X_test))\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2e486",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca11659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lf-l | 0.0478  0.002 | 62.5750 5.235 | 1.0000  0.000 | 5.7661  0.421\n",
      "an | 0.3087  0.049 | 7.3600 1.242 | 0.9686  0.019 | 63.0041  17.937\n",
      "ch | 0.7065  0.054 | 2.0900 0.453 | 0.9411  0.027 | 2.3512  0.189\n"
     ]
    }
   ],
   "source": [
    "full_coverage = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "rule_length = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_precision = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_time = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
