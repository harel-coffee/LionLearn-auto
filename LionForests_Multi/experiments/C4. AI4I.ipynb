{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f390a87d",
   "metadata": {},
   "source": [
    "## LionForests Multi Label Experiments: AI4I Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6e1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "cpath = !pwd\n",
    "sys.path.append('/usr/src/app/algorithms/')\n",
    "sys.path.append('/usr/src/app/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3ba77",
   "metadata": {},
   "source": [
    "Load Libraties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09530df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "from algorithms.anchor.anchor_tabular import AnchorTabularExplainer\n",
    "from datasets.dataset import Dataset\n",
    "import pandas as pd\n",
    "from utilities.dummy_utilizer import DummyUtilizer\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from algorithms.MARLENA.marlena.marlena.marlena import MARLENA\n",
    "from algorithms.simpleSurrogate import GlobalSurrogateTree, LocalSurrogateTree\n",
    "from algorithms.CHIRPS.structures import data_container\n",
    "import algorithms.CHIRPS.routines as rt\n",
    "import algorithms.CHIRPS.structures as strcts\n",
    "from scipy import sparse\n",
    "\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee0c5b",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05d1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai4i = Dataset()\n",
    "X, y, feature_names, label_names = ai4i.load_ai4i()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64f827",
   "metadata": {},
   "source": [
    "Initialize LionForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf03b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionforestsmulti import LionForests\n",
    "lf = LionForests(None, False, None, feature_names, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b949349",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{\n",
    "    'max_depth': [10],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [False],\n",
    "    'min_samples_leaf' : [1],\n",
    "    'n_estimators': [500]\n",
    "}]\n",
    "lf.fit(X, y, parameters, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e4b27",
   "metadata": {},
   "source": [
    "Best RF model for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f3b44cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=10, max_features='sqrt',\n",
       "                       n_estimators=500, n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412bfd1",
   "metadata": {},
   "source": [
    "Example explanation for option \"all predicted labelsets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8694d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 298.55<=Air temperature [K]<=298.95 & 308.85<=Process temperature [K]<=308.95 & 1448.0<=Rotational speed [rpm]<=1468.0 & 33.2<=Torque [Nm]<=41.45 & 207.5<=Tool wear [min]<=211.5 then TWF'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'all',True, method='1')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675013e",
   "metadata": {},
   "source": [
    "Example explanation for option \"per label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f47b6432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 298.55<=Air temperature [K]<=298.95 & 308.85<=Process temperature [K]<=308.95 & 1448.0<=Rotational speed [rpm]<=1468.0 & 33.2<=Torque [Nm]<=41.45 & 207.5<=Tool wear [min]<=211.5 then TWF'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'per label',True, method='1')['TWF'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ede34",
   "metadata": {},
   "source": [
    "Example explanation for option \"frequent pairs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc1f64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 298.55<=Air temperature [K]<=298.95 & 308.85<=Process temperature [K]<=308.95 & 1448.0<=Rotational speed [rpm]<=1468.0 & 33.2<=Torque [Nm]<=41.45 & 207.5<=Tool wear [min]<=211.5 then TWF'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X[2],'frequent pairs',7, True, method='1')[('TWF',)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b94335",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da18ef",
   "metadata": {},
   "source": [
    "1. Comparison between LF variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, class_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': ['sqrt'],\n",
    "        'bootstrap': [False],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [500]\n",
    "    }]\n",
    "    lf = LionForests(None, False, None, feature_names, class_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, class_names, lf, task, random_state=10):\n",
    "\n",
    "        # LionForests\n",
    "        def lf_rule_all(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'all')[5]\n",
    "            rule = {}\n",
    "            for key, value in temp.items():\n",
    "                rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "            return rule\n",
    "\n",
    "        def lf_rule_per_label(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'per label')\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        def lf_rule_pairs(instance):\n",
    "            temp = lf.explain_n_wise(\n",
    "                instance, 'frequent pairs', len(class_names))\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        return {'lf-a': lf_rule_all, 'lf-l': lf_rule_per_label, 'lf-p': lf_rule_pairs}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions, test, feature_names, class_names, lf,\n",
    "                                'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-a': 0, 'lf-l': 0, 'lf-p': 0}\n",
    "    rule_length = {'lf-a': 0, 'lf-l': 0, 'lf-p': 0}\n",
    "    f_precision = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "    time_response = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "    rules = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        # or trees in lf.model.estimators_:\n",
    "        #   print(trees.predict_proba([x_test_temp_lf[tesd_ind]]))\n",
    "        # rint(y_test_temp[tesd_ind])\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "            #print(tesd_ind,'sunexise madam')\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-a':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(y_test_temp[tesd_ind]), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp_lf)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                elif name == 'lf-l':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'lf-p':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        labeles = []\n",
    "                        for kk in key:\n",
    "                            labeles.append(list(label_names).index(kk))\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                temp_prediction = [y_test_temp[tesd_ind][j] for j in range(\n",
    "                                    len(y_test_temp[tesd_ind])) if j in labeles]\n",
    "                                temperatura = [y_test_temp[co][j] for j in range(\n",
    "                                    len(y_test_temp[co])) if j in labeles]\n",
    "                                precision.append(\n",
    "                                    [temp_prediction, temperatura])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, 0], precision[:, 1], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b99460",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10, random_state = 77)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    test_size.append(len(X_test)-results[-1])\n",
    "    total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c88a2f",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_coverage = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "rule_length = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "f_precision ={'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "f_time = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4e38b",
   "metadata": {},
   "source": [
    "2. Comparison between LF-a and multi-label explainability techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, class_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': ['sqrt'],\n",
    "        'bootstrap': [False],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [500]\n",
    "    }]\n",
    "    lf = LionForests(None, False, None, feature_names, class_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, class_names, lf, task, random_state=10):\n",
    "\n",
    "        # LionForests\n",
    "        def lf_rule_all(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'all')[5]\n",
    "            rule = {}\n",
    "            for key, value in temp.items():\n",
    "                rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "            return rule\n",
    "\n",
    "        gt = GlobalSurrogateTree(\n",
    "            train, predictions, feature_names, task, random_state)\n",
    "        print('    GT Ready')\n",
    "        lt = LocalSurrogateTree(\n",
    "            train, predictions, feature_names, task, 150, random_state)\n",
    "\n",
    "        def marlena(instance):\n",
    "            m1 = MARLENA(neigh_type='mixed', random_state=42)\n",
    "            i2e = pd.Series(instance, index=feature_names)\n",
    "            X2E = pd.DataFrame(train, columns=feature_names)\n",
    "            rule, _, _, _, _ = m1.extract_explanation(i2e, X2E, lf.model, feature_names, [],\n",
    "                                                      class_names, k=10, size=50, alpha=0.7)\n",
    "\n",
    "            path = rule.split('->')[0][1:-2]\n",
    "            prediction = rule.split('->')[1][1:]\n",
    "            conjuctions = {}\n",
    "            if '{}' in rule:\n",
    "                print(rule)\n",
    "            else:\n",
    "                for conj in path.split('\\n'):\n",
    "                    temp_conj = conj.strip(',')\n",
    "                    if temp_conj[0] == ' ':\n",
    "                        temp_conj = temp_conj[1:]\n",
    "                    if '>' in temp_conj:\n",
    "                        temp = temp_conj.split(' > ')\n",
    "                        if temp[0] in conjuctions:\n",
    "                            position = 0\n",
    "                            if len(conjuctions[temp[0]]) == 2:\n",
    "                                if conjuctions[temp[0]][1][0] == '>':\n",
    "                                    position = 1\n",
    "                            if conjuctions[temp[0]][position][0] == '>':\n",
    "                                if conjuctions[temp[0]][position][1] > float(temp[1]):\n",
    "                                    conjuctions[temp[0]][position][1] = float(\n",
    "                                        temp[1])\n",
    "                            elif conjuctions[temp[0]][0][0] == '<=':\n",
    "                                conjuctions[temp[0]].append(\n",
    "                                    ['>', float(temp[1])])\n",
    "\n",
    "                        else:\n",
    "                            conjuctions[temp[0]] = [['>', float(temp[1])]]\n",
    "                    else:\n",
    "                        temp = temp_conj.split(' <= ')\n",
    "                        if temp[0] in conjuctions:\n",
    "                            position = 0\n",
    "                            if len(conjuctions[temp[0]]) == 2:\n",
    "                                if conjuctions[temp[0]][1][0] == '<=':\n",
    "                                    position = 1\n",
    "\n",
    "                            if conjuctions[temp[0]][position][0] == '<=':\n",
    "                                if conjuctions[temp[0]][position][1] <= float(temp[1]):\n",
    "                                    conjuctions[temp[0]][position][1] = float(\n",
    "                                        temp[1])\n",
    "                            elif conjuctions[temp[0]][0][0] == '>':\n",
    "                                conjuctions[temp[0]].append(\n",
    "                                    ['<=', float(temp[1])])\n",
    "                        else:\n",
    "                            conjuctions[temp[0]] = [['<=', float(temp[1])]]\n",
    "            local_prediction = np.zeros((len(label_names)), dtype=int)\n",
    "            for label in range(len(label_names)):\n",
    "                if label_names[label] in prediction:\n",
    "                    local_prediction[label] = 1\n",
    "            return (conjuctions, local_prediction)\n",
    "\n",
    "        return {'lf-a': lf_rule_all, 'gs': gt.rule, 'ls': lt.rule, 'ma': marlena}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions, test, feature_names, class_names, lf,\n",
    "                                'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-a': 0, 'ls': 0, 'gs': 0, 'ma': 0}\n",
    "    rule_length = {'lf-a': 0, 'ls': 0, 'gs': 0, 'ma': 0}\n",
    "    f_precision = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "    time_response = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "    rules = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        # or trees in lf.model.estimators_:\n",
    "        #   print(trees.predict_proba([x_test_temp_lf[tesd_ind]]))\n",
    "        # rint(y_test_temp[tesd_ind])\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "            #print(tesd_ind,'sunexise madam')\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-a':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(y_test_temp[tesd_ind]), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp_lf)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                elif name == 'ls' or name == 'gs' or name == 'ma':\n",
    "                    ts = time.time()\n",
    "                    rule, prediction = method(x_test_temp[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp:\n",
    "                        res = rule_cov(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(prediction), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10, random_state = 77)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    test_size.append(len(X_test)-results[-1])\n",
    "    total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44114112",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155439cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_coverage = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "rule_length = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "f_precision ={'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "f_time = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dee6c",
   "metadata": {},
   "source": [
    "3. Comparison between LF-l and \"per label\" explainability techniques (CHIRPS/ANCHORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, class_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': ['sqrt'],\n",
    "        'bootstrap': [False],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [500]\n",
    "    }]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    lf = LionForests(None, False, scaler, feature_names, class_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, class_names, lf, task, random_state=10):\n",
    "\n",
    "        def lf_rule_per_label(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'per label')\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        inddd = 0\n",
    "        anchors = {}\n",
    "        for label_name in label_names:\n",
    "            explainer = AnchorTabularExplainer(\n",
    "                ['not '+label_name, label_name], feature_names, train)\n",
    "            anchors[inddd] = explainer\n",
    "            inddd += 1\n",
    "\n",
    "        def anchors_method(instance):\n",
    "            prediction = lf.model.predict([instance])[0]\n",
    "            rules = {}\n",
    "            for pred in range(len(prediction)):\n",
    "                if prediction[pred] == 1:\n",
    "                    explainer = anchors[pred]\n",
    "\n",
    "                    def my_model(x):\n",
    "                        return lf.model.predict(x)[:, pred]\n",
    "                    exp = explainer.explain_instance(\n",
    "                        instance, my_model, threshold=0.95)\n",
    "                    anchors_dict = {}\n",
    "                    for i in exp.names():\n",
    "                        terms = i.split(' ')\n",
    "                        if len(terms) == 3:\n",
    "                            anchors_dict[terms[0]] = [\n",
    "                                [terms[1], float(terms[2])]]\n",
    "                        else:\n",
    "                            anchors_dict[terms[2]] = [[terms[3], float(terms[4])], [\n",
    "                                terms[1], float(terms[0])]]\n",
    "                    rules[class_names[pred]] = anchors_dict\n",
    "            return rules\n",
    "\n",
    "        chts = time.time()\n",
    "        chirps = {}\n",
    "        inddd = 0\n",
    "        for label_name in label_names:\n",
    "            explainer = AnchorTabularExplainer(\n",
    "                ['not '+label_name, label_name], feature_names, train)\n",
    "            # CHIRPS =======================================================================================\n",
    "            project_dir = 'C:\\\\Users\\\\iamollas\\\\Downloads\\\\LionForests Journal\\\\algorithms\\\\CHIRPS'\n",
    "            temp_frame = pd.DataFrame(np.hstack((train, y_train[:, inddd].reshape(\n",
    "                len(y_train), 1))), columns=feature_names+['class'])\n",
    "            temp_frame['class'] = temp_frame['class'].astype(int)\n",
    "            #temp_frame = temp_frame.replace({\"class\": {1: 2}})\n",
    "            #temp_frame = temp_frame.replace({\"class\": {0: 1}})\n",
    "\n",
    "            mydata = data_container(\n",
    "                data=temp_frame, class_col='class', var_names=feature_names,\n",
    "                project_dir=project_dir, save_dir='flags'+str(inddd), random_state=random_state)\n",
    "            meta_data = mydata.get_meta()\n",
    "            f_walker = strcts.classification_trees_walker(\n",
    "                forest=model, inddd=inddd, meta_data=meta_data)\n",
    "            f_walker.forest_walk(instances=test, labels=model.predict(test)[\n",
    "                                 :, inddd], forest_walk_async=False)\n",
    "\n",
    "            explanations = strcts.CHIRPS_container(f_walker.path_detail,\n",
    "                                                   forest=model,\n",
    "                                                   # any representative sample can be used\n",
    "                                                   sample_instances=sparse.csr_matrix(\n",
    "                                                       train),\n",
    "                                                   sample_labels=predictions[:, inddd],\n",
    "                                                   meta_data=meta_data)\n",
    "            explanations.run_explanations(target_classes=model.predict(test)[:, inddd],  # we're explaining the prediction, not the true label!\n",
    "                                          explanation_async=False,\n",
    "                                          random_state=random_state,\n",
    "                                          which_trees='majority',\n",
    "                                          alpha_paths=0.0,\n",
    "                                          support_paths=0.1,\n",
    "                                          score_func=1,\n",
    "                                          precis_threshold=0.9,\n",
    "                                          disc_path_bins=4,\n",
    "                                          merging_bootstraps=20,\n",
    "                                          pruning_bootstraps=20,\n",
    "                                          delta=0.2,\n",
    "                                          weighting='kldiv')\n",
    "            chirps[inddd] = explanations\n",
    "            inddd += 1\n",
    "        chte = (time.time() - chts)/len(test)\n",
    "\n",
    "        def chirps_method(instance, idx):\n",
    "            prediction = lf.model.predict([instance])[0]\n",
    "            rules = {}\n",
    "            for pred in range(len(prediction)):\n",
    "                if prediction[pred] == 1:\n",
    "                    chirps_dict = {}\n",
    "                    explanations = chirps[pred]\n",
    "                    for i in explanations.explainers[idx].pruned_rule:\n",
    "                        if i[1]:\n",
    "                            chirps_dict[i[0]] = [['<=', float(i[2])]]\n",
    "                        else:\n",
    "                            chirps_dict[i[0]] = [['>', float(i[2])]]\n",
    "                    rules[class_names[pred]] = chirps_dict\n",
    "            return rules, 0, chte\n",
    "\n",
    "        return {'lf-l': lf_rule_per_label, 'an': anchors_method, 'ch': chirps_method}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions,\n",
    "                                test, feature_names, class_names, lf, 'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-l': 0, 'an': 0, 'ch': 0}\n",
    "    rule_length = {'lf-l': 0, 'an': 0, 'ch': 0}\n",
    "    f_precision = {'lf-l': [], 'an': [], 'ch': []}\n",
    "    time_response = {'lf-l': [], 'an': [], 'ch': []}\n",
    "    rules = {'lf-l': [], 'an': [], 'ch': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-l':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'an':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp:\n",
    "                            res = rule_cov(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'ch':\n",
    "                    rule, op, te = method(x_test_temp[tesd_ind], tesd_ind)\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp:\n",
    "                            res = rule_cov(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a920449",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10, random_state = 77)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, [i.replace(' ','_') for i in feature_names], label_names)\n",
    "    test_size.append(len(X_test)-results[-1])\n",
    "    total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9947a",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_coverage = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "rule_length = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_precision = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_time = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc7c60",
   "metadata": {},
   "source": [
    "## Qualitative Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9064edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2164fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.dummy_utilizer import DummyUtilizer\n",
    "lf = LionForests(None, False, DummyUtilizer(), feature_names, label_names)#,'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3558b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{\n",
    "    'max_depth': [5],\n",
    "    'max_features': [0.75],\n",
    "    'bootstrap': [False],\n",
    "    'min_samples_leaf' : [1],\n",
    "    'n_estimators': [500]\n",
    "}]\n",
    "lf.fit(x_train, y_train, parameters, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890d2ee",
   "metadata": {},
   "source": [
    "For a specific instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "726ec24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 84\n",
    "instance = x_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d98d6",
   "metadata": {},
   "source": [
    "Let's print the prediction, and the actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d91574d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 1, 1]), [1, 0, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.model.predict(lf.utilizer.transform(x_train))[idx], y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a8c3f",
   "metadata": {},
   "source": [
    "We now print the LF rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7dc127b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if 2.5<=Type<=3.0 & 300.6<=Air temperature [K]<=301.65 & 310.05<=Process temperature [K]<=313.7 & 1351.0<=Rotational speed [rpm]<=1380.0 & 65.2<=Torque [Nm]<=65.5 & 207.5<=Tool wear [min]<=209.0 then TWF PWF OSF',\n",
       " 500,\n",
       " 6,\n",
       " 251,\n",
       " 6,\n",
       " {'Type': [2.5, 3.0],\n",
       "  'Air temperature [K]': [300.6000061035156, 301.65000915527344],\n",
       "  'Process temperature [K]': [310.0500030517578, 313.7],\n",
       "  'Rotational speed [rpm]': [1351.0, 1380.0],\n",
       "  'Torque [Nm]': [65.20000076293945, 65.5],\n",
       "  'Tool wear [min]': [207.5, 209.0]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(instance,'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd323a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TWF': ['if 2.5<=Type<=3.0 & 295.6<=Air temperature [K]<=301.65 & 1322.5<=Rotational speed [rpm]<=1419.5 & 65.2<=Torque [Nm]<=76.2 & 206.5<=Tool wear [min]<=251.0 then TWF',\n",
       "  500,\n",
       "  6,\n",
       "  251,\n",
       "  5,\n",
       "  {'Type': [2.5, 3.0],\n",
       "   'Air temperature [K]': [295.6, 301.65000915527344],\n",
       "   'Rotational speed [rpm]': [1322.5, 1419.5],\n",
       "   'Torque [Nm]': [65.20000076293945, 76.2],\n",
       "   'Tool wear [min]': [206.5, 251.0]}],\n",
       " 'PWF': ['if 2.5<=Type<=3.0 & 295.6<=Air temperature [K]<=301.65 & 1351.0<=Rotational speed [rpm]<=1380.0 & 65.2<=Torque [Nm]<=76.2 & 188.0<=Tool wear [min]<=251.0 then PWF',\n",
       "  500,\n",
       "  6,\n",
       "  270,\n",
       "  5,\n",
       "  {'Type': [2.5, 3.0],\n",
       "   'Air temperature [K]': [295.6, 301.65000915527344],\n",
       "   'Rotational speed [rpm]': [1351.0, 1380.0],\n",
       "   'Torque [Nm]': [65.20000076293945, 76.2],\n",
       "   'Tool wear [min]': [188.0, 251.0]}],\n",
       " 'OSF': ['if 2.5<=Type<=3.0 & 300.6<=Air temperature [K]<=300.75 & 65.2<=Torque [Nm]<=65.5 & 207.5<=Tool wear [min]<=251.0 then OSF',\n",
       "  500,\n",
       "  6,\n",
       "  251,\n",
       "  4,\n",
       "  {'Type': [2.5, 3.0],\n",
       "   'Air temperature [K]': [300.6000061035156, 300.75],\n",
       "   'Torque [Nm]': [65.20000076293945, 65.5],\n",
       "   'Tool wear [min]': [207.5, 251.0]}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(instance,'per label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be6e7451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if 2.5<=Type<=3.0 & 295.6<=Air temperature [K]<=301.65 & 1351.0<=Rotational speed [rpm]<=1380.0 & 65.2<=Torque [Nm]<=76.2 & 185.0<=Tool wear [min]<=251.0 then PWF OSF',\n",
       " 500,\n",
       " 6,\n",
       " 270,\n",
       " 5,\n",
       " {'Type': [2.5, 3.0],\n",
       "  'Air temperature [K]': [295.6, 301.65000915527344],\n",
       "  'Rotational speed [rpm]': [1351.0, 1380.0],\n",
       "  'Torque [Nm]': [65.20000076293945, 76.2],\n",
       "  'Tool wear [min]': [185.0, 251.0]}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(instance,'frequent pairs', 10)[('PWF', 'OSF')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da3f10",
   "metadata": {},
   "source": [
    "And now the rules from the Global and Local Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc5d0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = GlobalSurrogateTree(x_train, lf.model.predict(x_train), feature_names, 'classification')\n",
    "lt = LocalSurrogateTree(x_train, lf.model.predict(x_train), feature_names, 'classification', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "a53b4730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Air temperature [K]': [['<=', 301.65000915527344]],\n",
       "  'Tool wear [min]': [['>', 176.5]],\n",
       "  'Torque [Nm]': [['>', 65.20000076293945]]},\n",
       " array([0, 0, 1, 1]))"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.rule(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52d0f761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Torque [Nm]': [['>', 48.400001525878906]],\n",
       "  'Tool wear [min]': [['>', 188.0]],\n",
       "  'Type': [['>', 2.5]]},\n",
       " array([1, 0, 1, 1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt.rule(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5eb351",
   "metadata": {},
   "source": [
    "We are also using MARLENA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "729c4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = MARLENA(neigh_type='mixed',random_state=42)\n",
    "i2e = pd.Series(instance,index=feature_names)\n",
    "X2E = pd.DataFrame(x_train,columns=feature_names)\n",
    "rule, _, _, _, _ = m1.extract_explanation(i2e ,X2E, lf.model, feature_names, [],\n",
    "                                                      label_names, k=10, size=50, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "53b0f7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{Air temperature [K] <= 303.0,\\n Type > 2.97,\\n Rotational speed [rpm] <= 1382.38} -> ['TWF' 'PWF' 'OSF']\""
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77707906",
   "metadata": {},
   "source": [
    "Now, we are extracting per label explanations from Anchors and CHIRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "9ba574ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inddd = 0\n",
    "anchors = {}\n",
    "for label_name in label_names:\n",
    "    explainer = AnchorTabularExplainer(['not '+label_name, label_name], feature_names, lf.utilizer.transform(X))\n",
    "    anchors[inddd] = explainer\n",
    "    inddd += 1        \n",
    "def anchors_method(instance):\n",
    "    prediction = lf.model.predict([instance])[0]\n",
    "    rules = []\n",
    "    for pred in range(len(prediction)):\n",
    "        if prediction[pred]==1:\n",
    "            explainer = anchors[pred]\n",
    "            def my_model(x):\n",
    "                return lf.model.predict(x)[:,pred]\n",
    "            exp = explainer.explain_instance(instance, my_model, threshold=0.95)\n",
    "            rules.append([label_names[pred],exp])\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "bee1ad1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TWF',\n",
       " ['Tool wear [min] > 207.50',\n",
       "  'Type > 2.00',\n",
       "  'Air temperature [K] <= 301.60',\n",
       "  'Torque [Nm] > 61.20',\n",
       "  'Rotational speed [rpm] <= 1365.00'])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_explanations = anchors_method(instance)\n",
    "anchor_explanations[0][0],anchor_explanations[0][1].exp_map['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "e49da1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PWF',\n",
       " ['Air temperature [K] <= 301.60',\n",
       "  'Torque [Nm] > 61.20',\n",
       "  'Type > 2.00',\n",
       "  '309.50 < Process temperature [K] <= 311.20',\n",
       "  '1326.50 < Rotational speed [rpm] <= 1365.00'])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_explanations[1][0],anchor_explanations[1][1].exp_map['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "93efe5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('OSF', ['Tool wear [min] > 207.50', 'Torque [Nm] > 61.20'])"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_explanations[2][0],anchor_explanations[2][1].exp_map['names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf3173",
   "metadata": {},
   "source": [
    "And CHIRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c1dd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chirps = {}\n",
    "inddd = 0 \n",
    "y_train = np.array(y_train)\n",
    "x_train = np.float32(x_train)\n",
    "for label_name in label_names:\n",
    "    #CHIRPS =======================================================================================\n",
    "    project_dir = 'C:\\\\Users\\\\iamollas\\\\Downloads\\\\LionForests Journal\\\\algorithms\\\\CHIRPS'\n",
    "    temp_frame = pd.DataFrame(np.hstack((x_train, y_train[:,inddd].reshape(len(y_train),1))),\n",
    "                              columns=feature_names+['class'])\n",
    "    temp_frame['class']=temp_frame['class'].astype(int)\n",
    "\n",
    "    mydata = data_container(\n",
    "            data = temp_frame, class_col = 'class', var_names = feature_names,\n",
    "            project_dir = project_dir, save_dir = 'flag'+str(inddd), random_state=123)\n",
    "    meta_data = mydata.get_meta()\n",
    "    f_walker = strcts.classification_trees_walker(forest=lf.model, inddd = inddd, meta_data=meta_data)\n",
    "    f_walker.forest_walk(instances = x_train, labels = lf.model.predict(x_train)[:,inddd], forest_walk_async=False)\n",
    "\n",
    "    explanations = strcts.CHIRPS_container(f_walker.path_detail,\n",
    "                                    forest=lf.model,\n",
    "                                    sample_instances=sparse.csr_matrix(x_train), # any representative sample can be used\n",
    "                                    sample_labels=lf.model.predict(x_train)[:,inddd],\n",
    "                                    meta_data=meta_data)\n",
    "    explanations.run_explanations(target_classes=lf.model.predict(x_train)[:,inddd], # we're explaining the prediction, not the true label!\n",
    "                            explanation_async=False,\n",
    "                            random_state=123,\n",
    "                            which_trees='majority',\n",
    "                            alpha_paths=0.0,\n",
    "                            support_paths=0.1,\n",
    "                            score_func=1,\n",
    "                            precis_threshold=0.9,\n",
    "                            disc_path_bins=4,\n",
    "                            merging_bootstraps=20,\n",
    "                            pruning_bootstraps=20,\n",
    "                            delta=0.2,\n",
    "                            weighting='kldiv')\n",
    "    chirps[inddd] = explanations\n",
    "    inddd += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d5b52a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'TWF': {},\n",
       "  'PWF': {'Air temperature [K]': [['<=', 302.55]],\n",
       "   'Torque [Nm]': [['>', 65.03305]]},\n",
       "  'OSF': {'Tool wear [min]': [['>', 176.53688]],\n",
       "   'Torque [Nm]': [['>', 65.03456]]}},\n",
       " 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chirps_method(instance, idx):\n",
    "    prediction = lf.model.predict([instance])[0]\n",
    "    rules = {}\n",
    "    for pred in range(len(prediction)):\n",
    "        if prediction[pred]==1:\n",
    "            chirps_dict = {}\n",
    "            explanations = chirps[pred]\n",
    "            for i in explanations.explainers[idx].pruned_rule:\n",
    "                if i[1]:\n",
    "                    chirps_dict[i[0]] = [['<=',float(i[2])]]\n",
    "                else:\n",
    "                    chirps_dict[i[0]] = [['>',float(i[2])]]\n",
    "            rules[label_names[pred]] = chirps_dict\n",
    "    return rules, 0\n",
    "\n",
    "chirps_method(instance,idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8caefb0",
   "metadata": {},
   "source": [
    "Let's try to change a few values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "900d0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0 0 0 1] [1 0 1 1]\n",
      "1 [0 0 0 1] [1 0 1 1]\n",
      "2 [0 0 0 1] [1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Type:\n",
    "for val in range(0, 3, 1):\n",
    "    tc = instance.copy()\n",
    "    tc[0]= val\n",
    "    vv = lf.model.predict([tc, instance])\n",
    "    print(val, vv[0], vv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1a678e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301.7 [0 0 0 1] [1 0 1 1]\n",
      "301.8 [0 0 0 1] [1 0 1 1]\n",
      "301.9 [0 0 0 1] [1 0 1 1]\n",
      "302.0 [0 0 0 1] [1 0 1 1]\n",
      "302.1 [0 0 0 1] [1 0 1 1]\n",
      "302.2 [0 0 0 1] [1 0 1 1]\n",
      "302.3 [0 0 0 1] [1 0 1 1]\n",
      "302.4 [0 0 0 1] [1 0 1 1]\n",
      "302.5 [0 0 0 1] [1 0 1 1]\n",
      "302.6 [0 0 0 1] [1 0 1 1]\n",
      "302.7 [0 0 0 1] [1 0 1 1]\n",
      "302.8 [0 0 0 1] [1 0 1 1]\n",
      "302.9 [0 0 0 1] [1 0 1 1]\n",
      "303.0 [0 0 0 1] [1 0 1 1]\n",
      "303.1 [0 0 0 1] [1 0 1 1]\n",
      "303.2 [0 0 0 1] [1 0 1 1]\n",
      "303.3 [0 0 0 1] [1 0 1 1]\n",
      "303.4 [0 0 0 1] [1 0 1 1]\n",
      "303.5 [0 0 0 1] [1 0 1 1]\n",
      "303.6 [0 0 0 1] [1 0 1 1]\n",
      "303.7 [0 0 0 1] [1 0 1 1]\n",
      "303.8 [0 0 0 1] [1 0 1 1]\n",
      "303.9 [0 0 0 1] [1 0 1 1]\n",
      "304.0 [0 0 0 1] [1 0 1 1]\n",
      "304.1 [0 0 0 1] [1 0 1 1]\n",
      "304.2 [0 0 0 1] [1 0 1 1]\n",
      "304.3 [0 0 0 1] [1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Air temperature [K]:\n",
    "for val in list(range(2956, 3006, 1))+list(range(3013,3044,1)):\n",
    "    tc = instance.copy()\n",
    "    tc[1]= val/10\n",
    "    vv = lf.model.predict([tc, instance])\n",
    "    if 1 in (vv[0]-vv[1]) or -1 in (vv[0]-vv[1]):\n",
    "        print(val/10, vv[0],vv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb3e9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process temperature [K]:\n",
    "for val in list(range(3061, 3100, 1)):#+list(range(3137,3137,1)):\n",
    "    tc = instance.copy()\n",
    "    tc[2]= val/10\n",
    "    vv = lf.model.predict([tc, instance])\n",
    "    if 1 in (vv[0]-vv[1]) or -1 in (vv[0]-vv[1]):\n",
    "        print(val, vv[0],vv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "958e15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotational speed [rpm]:\n",
    "for val in list(range(1181, 1351)) + list(range(1380, 2886)):\n",
    "    tc = instance.copy()\n",
    "    tc[3]= val\n",
    "    vv = lf.model.predict([tc, instance])\n",
    "    if 1 in (vv[0]-vv[1]) or -1 in (vv[0]-vv[1]):\n",
    "        print(val, vv[0],vv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c10bd04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484 [1 0 0 0] [1 0 1 1]\n",
      "485 [0 0 0 1] [1 0 1 1]\n",
      "486 [0 0 0 1] [1 0 1 1]\n",
      "487 [0 0 0 1] [1 0 1 1]\n",
      "488 [0 0 0 1] [1 0 1 1]\n",
      "489 [0 0 0 1] [1 0 1 1]\n",
      "490 [0 0 0 1] [1 0 1 1]\n",
      "491 [0 0 0 1] [1 0 1 1]\n",
      "492 [0 0 0 1] [1 0 1 1]\n",
      "493 [0 0 0 1] [1 0 1 1]\n",
      "494 [0 0 0 1] [1 0 1 1]\n",
      "495 [0 0 0 1] [1 0 1 1]\n",
      "496 [0 0 0 1] [1 0 1 1]\n",
      "497 [0 0 0 1] [1 0 1 1]\n",
      "498 [0 0 0 1] [1 0 1 1]\n",
      "499 [0 0 0 1] [1 0 1 1]\n",
      "500 [0 0 0 1] [1 0 1 1]\n",
      "501 [0 0 0 1] [1 0 1 1]\n",
      "502 [0 0 0 1] [1 0 1 1]\n",
      "503 [0 0 0 1] [1 0 1 1]\n",
      "504 [0 0 0 1] [1 0 1 1]\n",
      "505 [0 0 0 1] [1 0 1 1]\n",
      "506 [0 0 0 1] [1 0 1 1]\n",
      "507 [0 0 0 1] [1 0 1 1]\n",
      "508 [0 0 0 1] [1 0 1 1]\n",
      "509 [0 0 0 1] [1 0 1 1]\n",
      "510 [0 0 0 1] [1 0 1 1]\n",
      "511 [0 0 0 1] [1 0 1 1]\n",
      "512 [0 0 0 1] [1 0 1 1]\n",
      "513 [0 0 0 1] [1 0 1 1]\n",
      "514 [0 0 0 1] [1 0 1 1]\n",
      "515 [0 0 0 1] [1 0 1 1]\n",
      "516 [0 0 0 1] [1 0 1 1]\n",
      "517 [0 0 0 1] [1 0 1 1]\n",
      "518 [0 0 0 1] [1 0 1 1]\n",
      "519 [0 0 0 1] [1 0 1 1]\n",
      "520 [0 0 0 1] [1 0 1 1]\n",
      "521 [0 0 0 1] [1 0 1 1]\n",
      "522 [0 0 0 1] [1 0 1 1]\n",
      "523 [0 0 0 1] [1 0 1 1]\n",
      "524 [0 0 0 1] [1 0 1 1]\n",
      "525 [0 0 0 1] [1 0 1 1]\n",
      "526 [0 0 0 1] [1 0 1 1]\n",
      "527 [0 0 0 1] [1 0 1 1]\n",
      "528 [0 0 0 1] [1 0 1 1]\n",
      "529 [0 0 0 1] [1 0 1 1]\n",
      "530 [0 0 0 1] [1 0 1 1]\n",
      "531 [0 0 0 1] [1 0 1 1]\n",
      "532 [0 0 0 1] [1 0 1 1]\n",
      "533 [0 0 0 1] [1 0 1 1]\n",
      "534 [0 0 0 1] [1 0 1 1]\n",
      "535 [0 0 0 1] [1 0 1 1]\n",
      "536 [0 0 0 1] [1 0 1 1]\n",
      "537 [0 0 0 1] [1 0 1 1]\n",
      "538 [0 0 0 1] [1 0 1 1]\n",
      "539 [0 0 0 1] [1 0 1 1]\n",
      "540 [0 0 0 1] [1 0 1 1]\n",
      "541 [0 0 0 1] [1 0 1 1]\n",
      "542 [0 0 0 1] [1 0 1 1]\n",
      "543 [0 0 0 1] [1 0 1 1]\n",
      "544 [0 0 0 1] [1 0 1 1]\n",
      "545 [0 0 0 1] [1 0 1 1]\n",
      "546 [0 0 0 1] [1 0 1 1]\n",
      "547 [0 0 0 1] [1 0 1 1]\n",
      "548 [0 0 0 1] [1 0 1 1]\n",
      "549 [0 0 0 1] [1 0 1 1]\n",
      "550 [0 0 0 1] [1 0 1 1]\n",
      "551 [0 0 0 1] [1 0 1 1]\n",
      "552 [0 0 0 1] [1 0 1 1]\n",
      "553 [0 0 0 1] [1 0 1 1]\n",
      "554 [0 0 0 1] [1 0 1 1]\n",
      "555 [0 0 0 1] [1 0 1 1]\n",
      "556 [0 0 0 1] [1 0 1 1]\n",
      "557 [0 0 0 1] [1 0 1 1]\n",
      "558 [0 0 0 1] [1 0 1 1]\n",
      "559 [0 0 0 1] [1 0 1 1]\n",
      "560 [0 0 0 1] [1 0 1 1]\n",
      "561 [0 0 0 1] [1 0 1 1]\n",
      "562 [0 0 0 1] [1 0 1 1]\n",
      "563 [0 0 0 1] [1 0 1 1]\n",
      "564 [0 0 0 1] [1 0 1 1]\n",
      "565 [0 0 0 1] [1 0 1 1]\n",
      "566 [0 0 0 1] [1 0 1 1]\n",
      "567 [0 0 0 1] [1 0 1 1]\n",
      "568 [0 0 0 1] [1 0 1 1]\n",
      "569 [0 0 0 1] [1 0 1 1]\n",
      "570 [0 0 0 1] [1 0 1 1]\n",
      "571 [0 0 0 1] [1 0 1 1]\n",
      "572 [0 0 0 1] [1 0 1 1]\n",
      "573 [0 0 0 1] [1 0 1 1]\n",
      "574 [0 0 0 1] [1 0 1 1]\n",
      "575 [0 0 0 1] [1 0 1 1]\n",
      "576 [0 0 0 1] [1 0 1 1]\n",
      "577 [0 0 0 1] [1 0 1 1]\n",
      "578 [0 0 0 1] [1 0 1 1]\n",
      "579 [0 0 0 1] [1 0 1 1]\n",
      "580 [0 0 0 1] [1 0 1 1]\n",
      "581 [0 0 0 1] [1 0 1 1]\n",
      "582 [0 0 0 1] [1 0 1 1]\n",
      "583 [0 0 0 1] [1 0 1 1]\n",
      "584 [0 0 0 1] [1 0 1 1]\n",
      "585 [0 0 0 1] [1 0 1 1]\n",
      "586 [0 0 0 1] [1 0 1 1]\n",
      "587 [0 0 0 1] [1 0 1 1]\n",
      "588 [0 0 0 1] [1 0 1 1]\n",
      "589 [0 0 0 1] [1 0 1 1]\n",
      "590 [0 0 0 1] [1 0 1 1]\n",
      "591 [0 0 0 1] [1 0 1 1]\n",
      "592 [0 0 0 1] [1 0 1 1]\n",
      "593 [0 0 0 1] [1 0 1 1]\n",
      "594 [0 0 0 1] [1 0 1 1]\n",
      "595 [0 0 0 1] [1 0 1 1]\n",
      "596 [0 0 0 1] [1 0 1 1]\n",
      "597 [0 0 0 1] [1 0 1 1]\n",
      "598 [0 0 0 1] [1 0 1 1]\n",
      "599 [0 0 0 1] [1 0 1 1]\n",
      "600 [0 0 0 1] [1 0 1 1]\n",
      "601 [0 0 0 1] [1 0 1 1]\n",
      "602 [0 0 0 1] [1 0 1 1]\n",
      "603 [0 0 0 1] [1 0 1 1]\n",
      "604 [0 0 0 1] [1 0 1 1]\n",
      "605 [0 0 0 1] [1 0 1 1]\n",
      "606 [0 0 0 1] [1 0 1 1]\n",
      "607 [0 0 0 1] [1 0 1 1]\n",
      "608 [0 0 0 1] [1 0 1 1]\n",
      "609 [0 0 0 1] [1 0 1 1]\n",
      "610 [0 0 0 1] [1 0 1 1]\n",
      "611 [0 0 0 1] [1 0 1 1]\n",
      "612 [0 0 0 1] [1 0 1 1]\n",
      "613 [0 0 0 1] [1 0 1 1]\n",
      "614 [0 0 0 1] [1 0 1 1]\n",
      "615 [0 0 0 1] [1 0 1 1]\n",
      "616 [0 0 0 1] [1 0 1 1]\n",
      "617 [0 0 0 1] [1 0 1 1]\n",
      "618 [0 0 0 1] [1 0 1 1]\n",
      "619 [0 0 0 1] [1 0 1 1]\n",
      "620 [0 0 0 1] [1 0 1 1]\n",
      "621 [0 0 0 1] [1 0 1 1]\n",
      "622 [0 0 0 1] [1 0 1 1]\n",
      "623 [0 0 0 1] [1 0 1 1]\n",
      "624 [0 0 0 1] [1 0 1 1]\n",
      "625 [0 0 0 1] [1 0 1 1]\n",
      "626 [0 0 0 1] [1 0 1 1]\n",
      "627 [0 0 0 1] [1 0 1 1]\n",
      "628 [0 0 0 1] [1 0 1 1]\n",
      "629 [0 0 0 1] [1 0 1 1]\n",
      "630 [0 0 0 1] [1 0 1 1]\n",
      "631 [0 0 0 1] [1 0 1 1]\n",
      "632 [0 0 0 1] [1 0 1 1]\n",
      "633 [0 0 0 1] [1 0 1 1]\n",
      "634 [0 0 0 1] [1 0 1 1]\n",
      "635 [0 0 0 1] [1 0 1 1]\n",
      "636 [0 0 0 1] [1 0 1 1]\n",
      "637 [0 0 0 1] [1 0 1 1]\n",
      "638 [0 0 0 1] [1 0 1 1]\n",
      "639 [0 0 0 1] [1 0 1 1]\n",
      "640 [0 0 0 1] [1 0 1 1]\n",
      "641 [0 0 0 1] [1 0 1 1]\n",
      "642 [0 0 0 1] [1 0 1 1]\n",
      "643 [0 0 0 1] [1 0 1 1]\n",
      "644 [0 0 0 1] [1 0 1 1]\n",
      "645 [0 0 0 1] [1 0 1 1]\n",
      "646 [0 0 0 1] [1 0 1 1]\n",
      "647 [0 0 0 1] [1 0 1 1]\n",
      "648 [0 0 0 1] [1 0 1 1]\n",
      "649 [0 0 0 1] [1 0 1 1]\n",
      "650 [0 0 0 1] [1 0 1 1]\n",
      "651 [0 0 0 1] [1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Torque [Nm]:\n",
    "for val in list(range(484, 652, 1))+list(range(655, 766, 1)):\n",
    "    tc = instance.copy()\n",
    "    tc[4]= val/10\n",
    "    vv = lf.model.predict([tc, instance])\n",
    "    if 1 in (vv[0]-vv[1]) or -1 in (vv[0]-vv[1]):\n",
    "        print(val, vv[0],vv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ff7507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176.5 [0 0 1 0] [1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Tool wear [min]:\n",
    "for val in list(range(1765, 2075, 1))+list(range(2084, 2531, 1)):\n",
    "    tc = instance.copy()\n",
    "    tc[5]= val/10\n",
    "    vv = lf.model.predict([tc, instance])\n",
    "    if 1 in (vv[0]-vv[1]) or -1 in (vv[0]-vv[1]):\n",
    "        print(val/10, vv[0],vv[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
