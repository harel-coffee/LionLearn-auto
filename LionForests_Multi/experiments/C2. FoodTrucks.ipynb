{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609baa0c",
   "metadata": {},
   "source": [
    "## LionForests Multi Label Experiments: FoodTrucks Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b436bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "cpath = !pwd\n",
    "sys.path.append('/usr/src/app/algorithms/')\n",
    "sys.path.append('/usr/src/app/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09530df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import Dataset\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utilities.dummy_utilizer import DummyUtilizer\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from algorithms.MARLENA.marlena.marlena.marlena import MARLENA\n",
    "from algorithms.simpleSurrogate import GlobalSurrogateTree, LocalSurrogateTree\n",
    "from algorithms.CHIRPS.structures import data_container\n",
    "import algorithms.CHIRPS.routines as rt\n",
    "import algorithms.CHIRPS.structures as strcts\n",
    "from scipy import sparse\n",
    "from algorithms.anchor.anchor_tabular import AnchorTabularExplainer\n",
    "\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480b1db",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6a5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "foodtrucks = Dataset()\n",
    "X_p, y, feature_names, label_names = foodtrucks.load_foodtrucks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1872a",
   "metadata": {},
   "source": [
    "Initialize LionForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf03b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionforestsmulti import LionForests\n",
    "lf = LionForests(None, False, None, list(feature_names), label_names, categorical_features=['x0','x1'])#,'x2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b949349",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{\n",
    "    'max_depth': [10],\n",
    "    'max_features': [0.75],\n",
    "    'bootstrap': [True],\n",
    "    'min_samples_leaf' : [1],\n",
    "    'n_estimators': [1000]\n",
    "}]\n",
    "lf.fit(X_p, y, parameters, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525ccc8",
   "metadata": {},
   "source": [
    "Best RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6abc4491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, max_features=0.75, n_estimators=1000,\n",
       "                       n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a00bf5",
   "metadata": {},
   "source": [
    "Example explanation for option \"all predicted labelsets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bdc5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 0.5<=frequency numeric<=1.0 & 15.0<=expenses numeric<=17.5 & 4.5<=taste numeric<=5.0 & 1.5<=hygiene numeric<=2.5 & 1.5<=menu numeric<=2.0 & 4.5<=presentation numeric<=5.0 & 2.5<=attendance numeric<=3.5 & 4.5<=ingredients numeric<=5.0 & 1.5<=place.to.sit numeric<=2.5 & 1.5<=takeaway numeric<=2.0 & 1.5<=variation numeric<=2.0 & 1.5<=stop.strucks numeric<=2.0 & 1.0<=schedule numeric<=1.5 & 1.5<=age.group numeric<=2.5 & 1.25<=scholarity numeric<=1.75 & 3.5<=average.income numeric<=4.5 & 0.5<=has.work numeric<=1.0 & x0_by_chance & x1_afternoon & 1.5<=gender<=2.0 & 2.5<=marital.status<=3.0 then street_food brazilian_food'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X_p[2],'all',True, method='1')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8977b66a",
   "metadata": {},
   "source": [
    "Example explanation for option \"per label\" for label \"street food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d2e6d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 0.5<=frequency numeric<=1.0 & 15.0<=expenses numeric<=17.5 & 4.5<=taste numeric<=5.0 & 1.5<=hygiene numeric<=2.5 & 1.5<=menu numeric<=2.5 & 4.5<=presentation numeric<=5.0 & 2.5<=attendance numeric<=3.5 & 4.5<=ingredients numeric<=5.0 & 1.5<=place.to.sit numeric<=2.5 & 1.5<=takeaway numeric<=2.0 & 1.5<=variation numeric<=2.0 & 1.5<=stop.strucks numeric<=2.5 & 1.0<=schedule numeric<=1.5 & 1.5<=age.group numeric<=2.5 & 1.25<=scholarity numeric<=1.75 & 3.5<=average.income numeric<=4.5 & 0.5<=has.work numeric<=1.0 & x0_by_chance & x1_afternoon & 1.5<=gender<=2.0 & 2.5<=marital.status<=3.0 then street_food'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X_p[2],'per label',True, method='1')['street_food'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe70b8b",
   "metadata": {},
   "source": [
    "Example explanation for option \"frequent pairs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba248a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if 0.5<=frequency numeric<=1.0 & 15.0<=expenses numeric<=17.5 & 4.5<=taste numeric<=5.0 & 1.5<=hygiene numeric<=2.5 & 1.5<=menu numeric<=2.5 & 4.5<=presentation numeric<=5.0 & 2.5<=attendance numeric<=3.5 & 4.5<=ingredients numeric<=5.0 & 1.5<=place.to.sit numeric<=2.5 & 1.5<=takeaway numeric<=2.0 & 1.5<=variation numeric<=2.0 & 1.5<=stop.strucks numeric<=2.5 & 1.0<=schedule numeric<=1.5 & 1.5<=age.group numeric<=2.5 & 1.25<=scholarity numeric<=1.75 & 3.5<=average.income numeric<=4.5 & 0.5<=has.work numeric<=1.0 & x0_by_chance & x1_afternoon & 1.5<=gender<=2.0 & 2.5<=marital.status<=3.0 then street_food'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.explain_n_wise(X_p[2],'frequent pairs',7, True, method='1')[('street_food',)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1b32f",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08b545",
   "metadata": {},
   "source": [
    "1. Comparison between LF variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': [0.75],\n",
    "        'bootstrap': [True],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [1000]\n",
    "    }]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    lf = LionForests(None, False, scaler, feature_names,\n",
    "                     label_names, categorical_features=['x0', 'x1'])\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        # LionForests\n",
    "        def lf_rule_all(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'all')[8]\n",
    "            rule = {}\n",
    "            for key, value in temp.items():\n",
    "                rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "            return rule\n",
    "\n",
    "        def lf_rule_per_label(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'per label')\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][8].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        def lf_rule_pairs(instance):\n",
    "            temp = lf.explain_n_wise(\n",
    "                instance, 'frequent pairs', len(label_names))\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][8].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "\n",
    "        return {'lf-a': lf_rule_all, 'lf-l': lf_rule_per_label, 'lf-p': lf_rule_pairs}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions, test, feature_names, label_names, lf,\n",
    "                                'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-a': 0, 'lf-l': 0, 'lf-p': 0}\n",
    "    rule_length = {'lf-a': 0, 'lf-l': 0, 'lf-p': 0}\n",
    "    f_precision = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "    time_response = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "    rules = {'lf-a': [], 'lf-l': [], 'lf-p': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "    for tesd_ind in range(len(test)):\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "            #print(tesd_ind,'sunexise madam')\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-a':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(y_test_temp[tesd_ind]), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp_lf)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                elif name == 'lf-l':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'lf-p':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        labeles = []\n",
    "                        for kk in key:\n",
    "                            labeles.append(list(label_names).index(kk))\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                temp_prediction = [y_test_temp[tesd_ind][j] for j in range(\n",
    "                                    len(y_test_temp[tesd_ind])) if j in labeles]\n",
    "                                temperatura = [y_test_temp[co][j] for j in range(\n",
    "                                    len(y_test_temp[co])) if j in labeles]\n",
    "                                precision.append(\n",
    "                                    [temp_prediction, temperatura])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, 0], precision[:, 1], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b99460",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X_p):\n",
    "    X_train, X_test = X_p[train_index], X_p[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    test_size.append(len(X_test)-results[-1])\n",
    "    total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c8651",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51367ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_coverage = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "rule_length = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "f_precision ={'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "f_time = {'lf-a':[], 'lf-l':[], 'lf-p':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b35ca9",
   "metadata": {},
   "source": [
    "2. Comparison between LF-a and multi-label explainability techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': [0.75],\n",
    "        'bootstrap': [True],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [1000]\n",
    "    }]\n",
    "    lf = LionForests(None, False, None, feature_names,\n",
    "                     label_names, ['x0', 'x1'])\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        # LionForests\n",
    "        def lf_rule_all(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'all')[8]\n",
    "            rule = {}\n",
    "            for key, value in temp.items():\n",
    "                rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "            return rule\n",
    "\n",
    "        gt = GlobalSurrogateTree(\n",
    "            train, predictions, feature_names, task, random_state)\n",
    "        print('    GT Ready')\n",
    "        lt = LocalSurrogateTree(\n",
    "            train, predictions, feature_names, task, 150, random_state)\n",
    "\n",
    "        def marlena(instance):\n",
    "            m1 = MARLENA(neigh_type='mixed', random_state=42)\n",
    "            i2e = pd.Series(instance, index=feature_names)\n",
    "            X2E = pd.DataFrame(train, columns=feature_names)\n",
    "            #num_feature_names = num_features + ordinal_categorical_features\n",
    "            #categorical_fs = list(preprocess.named_transformers_['one-hot'].named_steps['one_hot'].get_feature_names())\n",
    "            rule, _, _, _, _ = m1.extract_explanation(i2e, X2E, lf.model, feature_names, [],  # categorical_fs,\n",
    "                                                      label_names, k=10, size=50, alpha=0.7)\n",
    "\n",
    "            path = rule.split('->')[0][1:-2]\n",
    "            prediction = rule.split('->')[1][1:]\n",
    "            conjuctions = {}\n",
    "            print(path)\n",
    "            if '{}' in rule:\n",
    "                print(rule)\n",
    "            else:\n",
    "                for conj in path.split('\\n'):\n",
    "                    temp_conj = conj.strip(',')\n",
    "                    if temp_conj[0] == ' ':\n",
    "                        temp_conj = temp_conj[1:]\n",
    "                    if '>' in temp_conj:\n",
    "                        temp = temp_conj.split(' > ')\n",
    "                        if temp[0] in conjuctions:\n",
    "                            position = 0\n",
    "                            if len(conjuctions[temp[0]]) == 2:\n",
    "                                if conjuctions[temp[0]][1][0] == '>':\n",
    "                                    position = 1\n",
    "                            if conjuctions[temp[0]][position][0] == '>':\n",
    "                                if conjuctions[temp[0]][position][1] > float(temp[1]):\n",
    "                                    conjuctions[temp[0]][position][1] = float(\n",
    "                                        temp[1])\n",
    "                            elif conjuctions[temp[0]][0][0] == '<=':\n",
    "                                conjuctions[temp[0]].append(\n",
    "                                    ['>', float(temp[1])])\n",
    "\n",
    "                        else:\n",
    "                            conjuctions[temp[0]] = [['>', float(temp[1])]]\n",
    "                    else:\n",
    "                        temp = temp_conj.split(' <= ')\n",
    "                        if temp[0] in conjuctions:\n",
    "                            position = 0\n",
    "                            if len(conjuctions[temp[0]]) == 2:\n",
    "                                if conjuctions[temp[0]][1][0] == '<=':\n",
    "                                    position = 1\n",
    "\n",
    "                            if conjuctions[temp[0]][position][0] == '<=':\n",
    "                                if conjuctions[temp[0]][position][1] <= float(temp[1]):\n",
    "                                    conjuctions[temp[0]][position][1] = float(\n",
    "                                        temp[1])\n",
    "                            elif conjuctions[temp[0]][0][0] == '>':\n",
    "                                conjuctions[temp[0]].append(\n",
    "                                    ['<=', float(temp[1])])\n",
    "                        else:\n",
    "                            conjuctions[temp[0]] = [['<=', float(temp[1])]]\n",
    "            local_prediction = np.zeros((len(label_names)), dtype=int)\n",
    "            for label in range(len(label_names)):\n",
    "                if label_names[label] in prediction:\n",
    "                    local_prediction[label] = 1\n",
    "            return (conjuctions, local_prediction)\n",
    "\n",
    "        return {'lf-a': lf_rule_all, 'gs': gt.rule, 'ls': lt.rule, 'ma': marlena}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions, test, feature_names, label_names, lf,\n",
    "                                'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-a': 0, 'ls': 0, 'gs': 0, 'ma': 0}\n",
    "    rule_length = {'lf-a': 0, 'ls': 0, 'gs': 0, 'ma': 0}\n",
    "    f_precision = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "    time_response = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "    rules = {'lf-a': [], 'ls': [], 'gs': [], 'ma': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        # or trees in lf.model.estimators_:\n",
    "        #   print(trees.predict_proba([x_test_temp_lf[tesd_ind]]))\n",
    "        # rint(y_test_temp[tesd_ind])\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "            #print(tesd_ind,'sunexise madam')\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-a':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp_lf:\n",
    "                        res = rule_cov_LF(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(y_test_temp[tesd_ind]), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp_lf)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                elif name == 'ls' or name == 'gs' or name == 'ma':\n",
    "                    ts = time.time()\n",
    "                    rule, prediction = method(x_test_temp[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    coverage = 0\n",
    "                    precision = []\n",
    "                    co = 0\n",
    "                    for i in x_test_temp:\n",
    "                        res = rule_cov(i, feature_names, rule)\n",
    "                        coverage = coverage + res\n",
    "                        if res == 1:\n",
    "                            precision.append(\n",
    "                                [list(prediction), list(y_test_temp[co])])\n",
    "                        co = co + 1\n",
    "                    if len(precision) >= 1:\n",
    "                        precision = np.array(precision)\n",
    "                        f_precision[name].append(precision_score(\n",
    "                            precision[:, 0], precision[:, 1], average='micro'))\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        coverage/len(x_test_temp)\n",
    "                    rules[name].append(rule)\n",
    "                    len_rule = len(rule)\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return rule_generator, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = []\n",
    "kf = KFold(n_splits=10, random_state = 77)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X_p):\n",
    "    X_train, X_test = X_p[train_index], X_p[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "    results = measure(X_train, X_test, y_train, y_test, feature_names, label_names)\n",
    "    test_size.append(len(X_test)-results[-1])\n",
    "    total_results.append(results)\n",
    "    folds = folds + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203295e2",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_coverage = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "rule_length = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "f_precision ={'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "f_time = {'lf-a':[], 'ls':[], 'gs':[], 'ma':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name, method in i[0].items():\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name, method in total_results[0][0].items():\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec19774",
   "metadata": {},
   "source": [
    "3. Comparison between LF-l and \"per label\" explainability techniques (CHIRPS/ANCHORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e8a06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(X_train, X_test, y_train, y_test, feature_names, label_names, tech=False, random_state=10):\n",
    "    parameters = [{\n",
    "        'max_depth': [10],\n",
    "        'max_features': [0.75],\n",
    "        'bootstrap': [True],\n",
    "        'min_samples_leaf': [1],\n",
    "        'n_estimators': [1000]\n",
    "    }]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    lf = LionForests(None, False, scaler, feature_names, label_names)\n",
    "    lf.fit(X_train, y_train, params=parameters)\n",
    "\n",
    "    train = lf.utilizer.transform(X_train)\n",
    "    test = lf.utilizer.transform(X_test)\n",
    "\n",
    "    predictions = lf.model.predict(train)\n",
    "    test_predictions = lf.model.predict(test)\n",
    "\n",
    "    def techniques(model, train, y_train, predictions, test, feature_names, label_names, lf, task, random_state=10):\n",
    "\n",
    "        def lf_rule_per_label(instance):\n",
    "            temp = lf.explain_n_wise(instance, 'per label')\n",
    "            rules = {}\n",
    "            for key_o in list(temp.keys()):\n",
    "                rule = {}\n",
    "                for key, value in temp[key_o][5].items():\n",
    "                    rule[key] = [['<=', value[1]], ['>', value[0]]]\n",
    "                rules[key_o] = rule\n",
    "            return rules\n",
    "        inddd = 0\n",
    "        return {'lf-l': lf_rule_per_label}\n",
    "\n",
    "        chts = time.time()\n",
    "        chirps = {}\n",
    "        inddd = 0\n",
    "        for label_name in label_names:\n",
    "            project_dir = 'C:\\\\Users\\\\iamollas\\\\Downloads\\\\LionForests Journal\\\\algorithms\\\\CHIRPS'\n",
    "            temp_frame = pd.DataFrame(np.hstack((train, y_train[:, inddd].reshape(\n",
    "                len(y_train), 1))), columns=feature_names+['class'])\n",
    "            temp_frame['class'] = temp_frame['class'].astype(int)\n",
    "\n",
    "            mydata = data_container(\n",
    "                data=temp_frame, class_col='class', var_names=feature_names,\n",
    "                project_dir=project_dir, save_dir='FT'+str(inddd), random_state=random_state)\n",
    "            meta_data = mydata.get_meta()\n",
    "            f_walker = strcts.classification_trees_walker(\n",
    "                forest=model, inddd=inddd, meta_data=meta_data)\n",
    "            f_walker.forest_walk(instances=test, labels=model.predict(test)[\n",
    "                                 :, inddd], forest_walk_async=False)\n",
    "\n",
    "            explanations = strcts.CHIRPS_container(f_walker.path_detail,\n",
    "                                                   forest=model,\n",
    "                                                   # any representative sample can be used\n",
    "                                                   sample_instances=sparse.csr_matrix(\n",
    "                                                       train),\n",
    "                                                   sample_labels=predictions[:, inddd],\n",
    "                                                   meta_data=meta_data)\n",
    "            explanations.run_explanations(target_classes=model.predict(test)[:, inddd],  # we're explaining the prediction, not the true label!\n",
    "                                          explanation_async=False,\n",
    "                                          random_state=random_state,\n",
    "                                          which_trees='majority',\n",
    "                                          alpha_paths=0.0,\n",
    "                                          support_paths=0.1,\n",
    "                                          score_func=1,\n",
    "                                          precis_threshold=0.8,\n",
    "                                          disc_path_bins=2,\n",
    "                                          merging_bootstraps=10,\n",
    "                                          pruning_bootstraps=10,\n",
    "                                          delta=0.2)\n",
    "            chirps[inddd] = explanations\n",
    "            inddd += 1\n",
    "        chte = (time.time() - chts)/len(test)\n",
    "\n",
    "        def chirps_method(instance, idx):\n",
    "            prediction = lf.model.predict([instance])[0]\n",
    "            rules = {}\n",
    "            for pred in range(len(prediction)):\n",
    "                if prediction[pred] == 1:\n",
    "                    chirps_dict = {}\n",
    "                    explanations = chirps[pred]\n",
    "                    for i in explanations.explainers[idx].pruned_rule:\n",
    "                        if i[1]:\n",
    "                            chirps_dict[i[0]] = [['<=', float(i[2])]]\n",
    "                        else:\n",
    "                            chirps_dict[i[0]] = [['>', float(i[2])]]\n",
    "                    rules[label_names[pred]] = chirps_dict\n",
    "            return rules, 0, chte\n",
    "\n",
    "        return {'lf-l': lf_rule_per_label}\n",
    "\n",
    "        return {'lf-l': lf_rule_per_label, 'an': anchors_method, 'ch': chirps_method}\n",
    "\n",
    "    interpretation = techniques(lf.model, train, y_train, predictions,\n",
    "                                test, feature_names, label_names, lf, 'classification', random_state)\n",
    "    if tech:\n",
    "        return interpretation, lf\n",
    "\n",
    "    def rule_cov(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] <= rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    def rule_cov_LF(instance, feature_names, rule):\n",
    "        covered = True\n",
    "        for k in range(len(instance)):\n",
    "            feature = feature_names[k]\n",
    "            if feature in rule.keys():\n",
    "                if len(rule[feature]) == 2:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "                    if instance[k] < rule[feature][1][1]:  # THIS <=\n",
    "                        return 0\n",
    "                elif rule[feature][0][0] == '>':\n",
    "                    if instance[k] <= rule[feature][0][1]:\n",
    "                        return 0\n",
    "                else:\n",
    "                    if instance[k] > rule[feature][0][1]:  # <=\n",
    "                        return 0\n",
    "        return 1\n",
    "\n",
    "    rule_generator = interpretation\n",
    "    full_coverage = {'lf-l': 0, 'an': 0, 'ch': 0}\n",
    "    rule_length = {'lf-l': 0, 'an': 0, 'ch': 0}\n",
    "    f_precision = {'lf-l': [], 'an': [], 'ch': []}\n",
    "    time_response = {'lf-l': [], 'an': [], 'ch': []}\n",
    "    rules = {'lf-l': [], 'an': [], 'ch': []}\n",
    "\n",
    "    x_train_temp = train\n",
    "    x_test_temp = test\n",
    "\n",
    "    y_train_temp = predictions\n",
    "    y_test_temp = test_predictions\n",
    "\n",
    "    x_train_temp_lf = lf.utilizer.inverse_transform(x_train_temp)\n",
    "    x_test_temp_lf = lf.utilizer.inverse_transform(x_test_temp)\n",
    "\n",
    "    skippedkataskeywastwnplunthriousunistounskipmadame = 0\n",
    "\n",
    "    for tesd_ind in range(len(test)):\n",
    "        # or trees in lf.model.estimators_:\n",
    "        #   print(trees.predict_proba([x_test_temp_lf[tesd_ind]]))\n",
    "        # rint(y_test_temp[tesd_ind])\n",
    "        if np.sum(y_test_temp[tesd_ind]) == 0:\n",
    "            #print(tesd_ind,'Kataskeuastes plunthriwn sunistoun skip')\n",
    "            skippedkataskeywastwnplunthriousunistounskipmadame += 1\n",
    "        else:\n",
    "\n",
    "            for name, method in rule_generator.items():\n",
    "                if name == 'lf-l':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp_lf[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp_lf:\n",
    "                            res = rule_cov_LF(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp_lf)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'an':\n",
    "                    ts = time.time()\n",
    "                    rule = method(x_test_temp[tesd_ind])\n",
    "                    te = time.time() - ts\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp:\n",
    "                            res = rule_cov(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "                elif name == 'ch':\n",
    "                    rule, op, te = method(x_test_temp[tesd_ind], tesd_ind)\n",
    "                    total_coverage = 0\n",
    "                    total_precision = []\n",
    "                    len_rule = 0\n",
    "                    for key in rule.keys():\n",
    "                        temp_rule = rule[key]\n",
    "                        len_rule += len(temp_rule)\n",
    "                        label_index = list(label_names).index(key)\n",
    "                        coverage = 0\n",
    "                        precision = []\n",
    "                        co = 0\n",
    "                        for i in x_test_temp:\n",
    "                            res = rule_cov(i, feature_names, temp_rule)\n",
    "                            coverage = coverage + res\n",
    "                            if res == 1:\n",
    "                                precision.append(\n",
    "                                    [y_test_temp[tesd_ind][label_index], y_test_temp[co][label_index]])\n",
    "                            co = co + 1\n",
    "                        if len(precision) >= 1:\n",
    "                            precision = np.array(precision)\n",
    "                            total_precision.append(precision_score(\n",
    "                                precision[:, :1], precision[:, 1:], average='micro'))\n",
    "                        total_coverage += coverage/len(x_test_temp)\n",
    "                    total_precision = [k for k in total_precision if str(\n",
    "                        k) != str(np.average([]))]\n",
    "                    f_precision[name].append(np.array(total_precision).mean())\n",
    "                    full_coverage[name] = full_coverage[name] + \\\n",
    "                        total_coverage/len(rule.keys())\n",
    "                    rules[name].append(rule)\n",
    "\n",
    "                time_response[name].append(te)\n",
    "                rule_length[name] = rule_length[name] + len_rule\n",
    "    return _, full_coverage, rule_length, f_precision, time_response, skippedkataskeywastwnplunthriousunistounskipmadame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f19f60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names2 = [i.replace(' ','_') for i in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c477067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#total_results = []\n",
    "kf = KFold(n_splits=10, random_state=77)\n",
    "folds = 0\n",
    "test_size = []\n",
    "for train_index, test_index in kf.split(X_p):\n",
    "    if True:\n",
    "        X_train, X_test = X_p[train_index], X_p[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        test_size.append(len(X_test))\n",
    "        print('# of Fold: ' + str(folds+1) + ', size of test: ' + str(len(X_test)))    \n",
    "        results = measure(X_train, X_test, y_train, y_test, feature_names2, label_names)\n",
    "        total_results.append(results)\n",
    "    folds=folds+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b6b17",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da7c36b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lf-l | 0.0245  0.001 | 41.4805 4.763 | 1.0000  0.000 | 10.2610  0.895\n"
     ]
    }
   ],
   "source": [
    "full_coverage = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "rule_length = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_precision = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_time = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "k = 0\n",
    "for i in total_results[10:]:\n",
    "    for name in ['lf-l']:\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name in ['lf-l']:\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c14319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch | 0.7285  0.088 | 3.6631 1.114 | 0.8990  0.028 | 8.4652  0.353\n"
     ]
    }
   ],
   "source": [
    "full_coverage = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "rule_length = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_precision = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "f_time = {'lf-l':[], 'an':[], 'ch':[]}\n",
    "k = 0\n",
    "for i in total_results:\n",
    "    for name in ['ch']:\n",
    "        full_coverage[name].append(i[1][name]/test_size[k])\n",
    "        rule_length[name].append(i[2][name]/test_size[k])\n",
    "        l = [k for k in i[3][name] if str(k) != str(np.average([])) ]\n",
    "        f_precision[name].append(np.array(l).mean())\n",
    "        f_time[name].append(np.array(i[4][name]).mean())\n",
    "    k = + 1\n",
    "for name in ['ch']:\n",
    "    print(name,  '| %5.4f  %5.3f | %5.4f %5.3f | %5.4f  %5.3f | %5.4f  %5.3f' \n",
    "          % (np.array(full_coverage[name]).mean(),np.array(full_coverage[name]).std(),\n",
    "             np.array(rule_length[name]).mean(),np.array(rule_length[name]).std(),\n",
    "             np.array(f_precision[name]).mean(),np.array(f_precision[name]).std(),\n",
    "             np.array(f_time[name]).mean(),np.array(f_time[name]).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1886f171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an | 0.0642  0.016 | 8.9537 2.301 | 0.9859  0.017 | 341.6183  105.925\n"
     ]
    }
   ],
   "source": [
    "#ANCHORS:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
