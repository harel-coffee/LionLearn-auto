{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the experiments on Adult Census dataset with LionForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from lionforests import LionForests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the dataset and we set the feature and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmollas/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/johnmollas/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country','salary']\n",
    "class_names=['<=50K','>50K'] #0: <=50K and 1: >50K\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=feature_names, delimiter=', ')\n",
    "data_test = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test', names=feature_names, delimiter=', ')\n",
    "data_test = data_test.drop(data_test.index[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing the following preprocessing influenced by a github notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmollas/anaconda3/lib/python3.6/site-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "data = data[(data != '?').all(axis=1)]\n",
    "data_test = data_test[(data_test != '?').all(axis=1)]\n",
    "data_test['salary'] = data_test['salary'].map({'<=50K.': '<=50K', '>50K.': '>50K'})\n",
    "frames = [data, data_test]\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering from:\n",
    "https://github.com/pooja2512/Adult-Census-Income/blob/master/Adult%20Census%20Income.ipynb. So run and skip the next code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in final dataset: (45167, 82)\n"
     ]
    }
   ],
   "source": [
    "hs_grad = ['HS-grad','11th','10th','9th','12th']\n",
    "elementary = ['1st-4th','5th-6th','7th-8th']\n",
    "# replace elements in list.\n",
    "for i in hs_grad:\n",
    "    data['education'].replace(i , 'HS-grad', regex=True , inplace=True)\n",
    "for e in elementary:\n",
    "    data['education'].replace(e , 'elementary-school', regex=True, inplace = True)\n",
    "\n",
    "married= ['Married-spouse-absent','Married-civ-spouse','Married-AF-spouse']\n",
    "separated = ['Separated','Divorced']\n",
    "#replace elements in list.\n",
    "for m in married:\n",
    "    data['marital-status'].replace(m ,'Married', regex=True, inplace = True)\n",
    "for s in separated:\n",
    "    data['marital-status'].replace(s ,'Separated', regex=True, inplace = True)\n",
    "\n",
    "self_employed = ['Self-emp-not-inc','Self-emp-inc']\n",
    "govt_employees = ['Local-gov','State-gov','Federal-gov']\n",
    "for se in self_employed:\n",
    "    data['workclass'].replace(se , 'Self_employed', regex=True, inplace = True)\n",
    "for ge in govt_employees:\n",
    "    data['workclass'].replace(ge , 'Govt_employees', regex=True, inplace = True)\n",
    "\n",
    "del_cols = ['relationship','education-num']\n",
    "data.drop(labels = del_cols, axis = 1, inplace = True)\n",
    "\n",
    "index_age = data[data['age'] == 90].index\n",
    "data.drop(labels = index_age, axis = 0, inplace =True)\n",
    "num_col_new = ['age','capital-gain', 'capital-loss',\n",
    "       'hours-per-week','fnlwgt']\n",
    "cat_col_new = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "               'race', 'sex','salary','native-country']#add native-country label\n",
    "scaler = MinMaxScaler()\n",
    "#pd.DataFrame(scaler.fit_transform(data[num_col_new]),columns = num_col_new)\n",
    "class DataFrameSelector(TransformerMixin):\n",
    "    def __init__(self,attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self,X,y = None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.attribute_names]\n",
    "class num_trans(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        df = pd.DataFrame(X)\n",
    "        df.columns = num_col_new \n",
    "        return df\n",
    "pipeline = Pipeline([('selector',DataFrameSelector(num_col_new)),  \n",
    "                     ('scaler',MinMaxScaler()),('transform',num_trans())])#('scaler',MinMaxScaler()),        \n",
    "num_df = pipeline.fit_transform(data)\n",
    "num_df.shape\n",
    "# columns which I don't need after creating dummy variables dataframe\n",
    "cols = ['workclass_Govt_employess','education_Some-college',\n",
    "        'marital-status_Never-married','occupation_Other-service',\n",
    "        'race_Black','sex_Male','salary_>50K']\n",
    "class dummies(TransformerMixin):\n",
    "    def __init__(self,cols):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def fit(self,X,y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        df = pd.get_dummies(X)\n",
    "        df_new = df[df.columns.difference(cols)] \n",
    "        return df_new\n",
    "pipeline_cat=Pipeline([('selector',DataFrameSelector(cat_col_new)),\n",
    "                      ('dummies',dummies(cols))])\n",
    "cat_df = pipeline_cat.fit_transform(data)\n",
    "cat_df['id'] = pd.Series(range(cat_df.shape[0]))\n",
    "num_df['id'] = pd.Series(range(num_df.shape[0]))\n",
    "final_df = pd.merge(cat_df,num_df,how = 'inner', on = 'id')\n",
    "print(f\"Number of observations in final dataset: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the train and target data from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = final_df['salary_<=50K'].values\n",
    "final_df.drop(labels = ['id','salary_<=50K'],axis = 1,inplace = True)\n",
    "X = final_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need the new onehot encoded features' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = list(final_df.columns.values)\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'race', 'sex','native-country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the name of the categorical features in order LionForests to extract more compact explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   17.5s finished\n"
     ]
    }
   ],
   "source": [
    "my_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "parameters = [{\n",
    "    'max_depth': [10],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [False],\n",
    "    'min_samples_leaf' : [1],\n",
    "    'n_estimators': [100]\n",
    "}]\n",
    "lf = LionForests(class_names=class_names)\n",
    "lf.train(X, y, my_scaler, feature_names, None, categorical_features) #Please do not ascale data before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we use the build-in GridSearch of LionForests to find the best classifier for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8870718533523864 , Number of estimators: 100\n",
      "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                       max_depth=10, max_features='sqrt', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "number_of_estimators = lf.model.n_estimators\n",
    "print(\"Accuracy:\",lf.accuracy,\", Number of estimators:\",lf.number_of_estimators)\n",
    "print(lf.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract explanations about an instance. We choose the eleventh instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if marital-status_Married & sex_Female & education_HS-grad & workclass_Private & 0.012<=fnlwgt<=0.101 & 0.404<=age<=0.486 & 0.082<=hours-per-week<=0.24 & 0.0<=capital-gain<=0.022 & 0.0<=capital-loss<=0.432 & native-country_Jamaica then >50K'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.following_breadcrumbs(X[10], False, True, False, complexity=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original rule would have been this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if marital-status_Married & sex_Female & education_HS-grad & workclass_Private & 0.055<=fnlwgt<=0.1 & 0.404<=age<=0.486 & 0.117<=hours-per-week<=0.24 & 0.0<=capital-gain<=0.003 & 0.0<=capital-loss<=0.366 & native-country_Jamaica then >50K'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.following_breadcrumbs(X[10], False, False, False, complexity=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, some categorical alternative values got reduced. Currently, LionForests do not extract them automatically, but it will be implemented soon. You can notice that fnlwgt, age, hours per week and capital gain and loss values were not inverse transformed from LionForests. This is happening because we preprocessed this data externally. Thus we have to inverse transform them manually. If you want check instance 882 too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.247800e+01, 2.999970e+02, 1.463616e+03, 2.452000e+01,\n",
       "       3.827190e+05])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps.scaler.inverse_transform([[0.486,0.003,0.336,0.24,0.25]])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
