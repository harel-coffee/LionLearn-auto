{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LioNets: HateSpeech Dataset with Neural Networks and Embeddings-> Classification Task\n",
    "\n",
    "In this notebook, we present how LioNets can be applied in predictive models using embeddings as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlqliX9gdv8D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Testing a variety of NN architectures with Embeddings             #\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, CuDNNLSTM, Bidirectional, Dense, \\\n",
    "    LSTM, Conv1D, MaxPooling1D, Dropout, concatenate, Flatten, add, RepeatVector, ConvLSTM2D, TimeDistributed, Reshape\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import model_from_json\n",
    "from keras import objectives, backend as K\n",
    "from keras.engine import Layer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import Input, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',400)\n",
    "\n",
    "from lionets import LioNets\n",
    "from utilities.custom_attention import Attention\n",
    "from utilities.load_dataset import Load_Dataset\n",
    "from utilities.evaluation import Evaluation\n",
    "\n",
    "from altruist.altruist import Altruist\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "from innvestigate.utils.keras import checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sG8-UNmQAyrE",
    "outputId": "606dd583-084a-4d75-efab-81512dbe99c8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Load_Dataset.load_hate_data(True,False)\n",
    "#X_unsup,y_unsup = Preproccesor.load_unsupervised_data(True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['noHateSpeech', 'hateSpeech']\n",
    "X_train, X_valid, y_train, y_valid =  train_test_split(X,y,test_size=0.2, stratify = y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "xMD34FnoeN6H",
    "outputId": "e962fbe0-66ca-4ff5-8bec-198ca68a3145",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"/content/crawl-300d-2M.vec.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    print(zip_ref.filelist)\n",
    "del zip_ref\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGSO0Vv1fcN4"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SM-R5Y-7f10f"
   },
   "outputs": [],
   "source": [
    "def build_matrix(embedding_path, tk, max_features):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding = \"utf-8\"))\n",
    "\n",
    "    word_index = tk.word_index\n",
    "    nb_words = max_features\n",
    "    embedding_matrix = np.zeros((nb_words + 1, 50))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3cVmRJwe4gE"
   },
   "outputs": [],
   "source": [
    "embedding_path1 = \"embeddings/crawl-300d-2M.vec\" #FastText\n",
    "embedding_path1 = 'embeddings/glove.twitter.27B.50d.txt' #GloveSmall\n",
    "embed_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IcsGGos6hJ_0"
   },
   "outputs": [],
   "source": [
    "max_features = 500\n",
    "max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='', num_words=max_features, oov_token = True)\n",
    "tk.fit_on_texts(X_train)\n",
    "train_tokenized = tk.texts_to_sequences(X_train)\n",
    "valid_tokenized = tk.texts_to_sequences(X_valid)\n",
    "X_tr = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "X_va = pad_sequences(valid_tokenized, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Bf1Jsmiqe5J",
    "outputId": "77253145-1446-4164-ba59-87b3ec466cd2"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = build_matrix(embedding_path1, tk, max_features)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SuzKHET-uGsV"
   },
   "outputs": [],
   "source": [
    "train_y = [0.1 if i <=0.5 else 0.9 for i in y_train]\n",
    "valid_y = [0.1 if i <=0.5 else 0.9 for i in y_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gKuchEVbJot"
   },
   "outputs": [],
   "source": [
    "file_path = \"Hate_Predictor.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=2,save_best_only=True, mode=\"auto\")\n",
    "main_input = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
    "embedding_input = (Embedding(max_features + 1, 50, input_length=max_len,  weights=[embedding_matrix], trainable=False))(main_input)\n",
    "\n",
    "\n",
    "\n",
    "embedding_input2 = SpatialDropout1D(0.5)(embedding_input)\n",
    "\n",
    "x = Bidirectional(LSTM(100, return_sequences=True))(embedding_input2)\n",
    "encoder_x = concatenate([\n",
    "    Attention(max_len)(x),\n",
    "    GlobalMaxPooling1D()(x),\n",
    "])\n",
    "\n",
    "y = Conv1D(filters=100,kernel_size=3,activation='tanh')(embedding_input)\n",
    "encoder_y = GlobalMaxPooling1D()(y)\n",
    "\n",
    "hidden = concatenate([encoder_x,encoder_y])\n",
    "\n",
    "hidden = Dropout(0.5)(hidden)#0.5\n",
    "hidden = Dense(750, activation='tanh')(hidden)\n",
    "hidden = Dropout(0.7)(hidden)\n",
    "hidden = Dense(500, activation='tanh')(hidden)\n",
    "output_lay = Dense(1, activation='sigmoid')(hidden)\n",
    "model = Model(inputs=[main_input], outputs=[output_lay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wLKVVlb7lQE6",
    "outputId": "fc9c0a35-d279-4947-dd52-2887f5a65fdc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(X_tr, train_y, batch_size=128, epochs=200, validation_data=(X_va, valid_y), verbose=1, callbacks=[check_point], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJd3cKmevH9x"
   },
   "outputs": [],
   "source": [
    "weights_file = 'weights/Hate_Predictor.hdf5' # choose the best checkpoint few features\n",
    "model.load_weights(weights_file) # load it\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "temp_predo1 = model.predict(X_tr)\n",
    "predictions = [0 if i[0] <=0.5 else 1 for i in temp_predo1]\n",
    "print('Train:',f1_score(y_train,predictions, average='macro'),f1_score(y_train,predictions, average='weighted'),\n",
    "      balanced_accuracy_score(y_train,predictions),accuracy_score(y_train,predictions))\n",
    "\n",
    "temp_predo2 = model.predict(X_va)\n",
    "predictions = [0 if i[0] <=0.5 else 1 for i in temp_predo2]\n",
    "print('Train:',f1_score(y_valid,predictions, average='macro'),f1_score(y_valid,predictions, average='weighted'),\n",
    "      balanced_accuracy_score(y_valid,predictions), accuracy_score(y_valid,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "flIxO3vf1ziN",
    "outputId": "b21919ac-a6cd-4dd9-f6bb-4eca01a2f8a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input=model.input, output=[model.layers[-2].output])\n",
    "encoder.trainable = False\n",
    "encoder.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OoKuusf173A"
   },
   "outputs": [],
   "source": [
    "predictor_for_encoded = Sequential()\n",
    "predictor_for_encoded.add(model.layers[len(model.layers)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mEfpHTe2CJi"
   },
   "outputs": [],
   "source": [
    "encoded_x_train = encoder.predict(X_tr)\n",
    "encoded_x_valid = encoder.predict(X_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iw = tk.index_word.copy()\n",
    "iw[1]='UKN'\n",
    "X_T = []\n",
    "for i in X_tr:\n",
    "    X_T.append(' '.join([iw[o] for o in i if o !=0]))\n",
    "X_V = []\n",
    "for i in X_va:\n",
    "    X_V.append(' '.join([iw[o] for o in i if o !=0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[5],X_T[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_2 = max_features + 1\n",
    "temp = np.zeros((X_tr.shape[0], max_len, max_features_2))\n",
    "temp[np.expand_dims(np.arange(X_tr.shape[0]), axis=0).reshape(X_tr.shape[0], 1), \n",
    "     np.repeat(np.array([np.arange(max_len)]), X_tr.shape[0], axis=0), X_tr] = 1\n",
    "X_train_one_hot = temp\n",
    "\n",
    "temp = np.zeros((X_va.shape[0], max_len, max_features_2))\n",
    "temp[np.expand_dims(np.arange(X_va.shape[0]), axis=0).reshape(X_va.shape[0], 1), \n",
    "     np.repeat(np.array([np.arange(max_len)]), X_va.shape[0], axis=0), X_va] = 1\n",
    "X_valid_one_hot = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(encoded_x_train[0].shape))\n",
    "hidden = RepeatVector(50)(encoded_input)\n",
    "decoded = LSTM(350, return_sequences=True)(hidden)\n",
    "decoded = LSTM(750, return_sequences=True, name='dec_lstm_2')(decoded)\n",
    "decoded = TimeDistributed(Dense(max_features_2, activation='softmax'), name='decoded_mean')(decoded)\n",
    "\n",
    "z_mean = Dense(500, name='z_mean', activation='linear')(encoded_input)\n",
    "z_log_var = Dense(500, name='z_log_var', activation='linear')(encoded_input)\n",
    "\n",
    "decoder = Model(encoded_input,decoded)\n",
    "decoder.summary()\n",
    "decoder.compile(optimizer=\"Adam\",loss=['categorical_crossentropy'],metrics=['mae'])\n",
    "\n",
    "checkpoint_name = 'Hate_Decoder.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 2, save_best_only = True, mode ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#decoder.fit( np.concatenate((encoded_x_train,encoded_x_valid)), np.concatenate((X_train_one_hot,X_valid_one_hot)), \n",
    "#             epochs=1000, batch_size=128, shuffle=True, \n",
    "#             validation_data=(np.concatenate((encoded_x_train,encoded_x_valid)), np.concatenate((X_train_one_hot,X_valid_one_hot))), \n",
    "#             verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = 'weights/Hate_Decoder.hdf5' # choose the best checkpoint few features\n",
    "decoder.load_weights(wights_file) # load it\n",
    "decoder.compile(optimizer=\"Adam\",loss=['categorical_crossentropy'],metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.evaluate(encoded_x_train,X_train_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.evaluate(encoded_x_valid,X_valid_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_evaluation = decoder.predict(encoded_x_train[40:50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(instances_evaluation)):\n",
    "    tempo = X_train_one_hot[j+40]\n",
    "    tempo_ind = []\n",
    "    tempo_str = \"\"\n",
    "    for i in tempo:\n",
    "        tempo_ind.append(np.argmax(i))\n",
    "        if np.argmax(i) != 0 and np.argmax(i)!=True:\n",
    "            tempo_str = tempo_str + tk.index_word[np.argmax(i)]+\" \"\n",
    "        elif np.argmax(i) == True:\n",
    "            tempo_str = tempo_str + 'UKN'+ \" \"\n",
    "    print(\" Original\",tempo_str)\n",
    "\n",
    "    tempo = instances_evaluation[j]\n",
    "    tempo_ind = []\n",
    "    tempo_str = \"\"\n",
    "    for i in tempo:\n",
    "        tempo_ind.append(np.argmax(i))\n",
    "        if np.argmax(i) != 0 and np.argmax(i)!=True:\n",
    "            #print(sorted(i,reverse=True)[:5])\n",
    "            #print(i.max(),tk.index_word[np.argmax(i)])\n",
    "            tempo_str = tempo_str + tk.index_word[np.argmax(i)]+\" \"\n",
    "        elif np.argmax(i) == True:\n",
    "            tempo_str = tempo_str + 'UKN'+ \" \"\n",
    "    print(\"  Decoded:\",tempo_str)\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_evaluation = decoder.predict(encoded_x_valid[10:20]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(instances_evaluation)):\n",
    "    tempo = X_valid_one_hot[j+10]\n",
    "    tempo_ind = []\n",
    "    tempo_str = \"\"\n",
    "    for i in tempo:\n",
    "        tempo_ind.append(np.argmax(i))\n",
    "        if np.argmax(i) != 0 and np.argmax(i)!=True:\n",
    "            tempo_str = tempo_str + tk.index_word[np.argmax(i)]+\" \"\n",
    "        elif np.argmax(i) == True:\n",
    "            tempo_str = tempo_str + 'UKN'+ \" \"\n",
    "    print(\" Original\",tempo_str)\n",
    "\n",
    "    tempo = instances_evaluation[j]\n",
    "    tempo_ind = []\n",
    "    tempo_str = \"\"\n",
    "    for i in tempo:\n",
    "        tempo_ind.append(np.argmax(i))\n",
    "        if np.argmax(i) != 0 and np.argmax(i)!=True:\n",
    "            tempo_str = tempo_str + tk.index_word[np.argmax(i)]+\" \"\n",
    "        elif np.argmax(i) == True:\n",
    "            tempo_str = tempo_str + 'UKN'+ \" \"\n",
    "    print(\"  Decoded:\",tempo_str)\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.index_word[1]='UKN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.word_index['UKN'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LioNets Experiments\n",
    "Having everything setted up, we are now ready to try our methodology, Gradient x Input and LIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, SGDRegressor, LinearRegression\n",
    "lionet = LioNets(model, decoder, encoder, X_tr, decoder_lower_threshold=0, double_detector=True, embeddings=True, tk=tk)\n",
    "transparent_model = Ridge(alpha=0.02,fit_intercept=True,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(2000)\n",
    "train = np.array(random.sample(list(X_tr),200))\n",
    "valid = np.array(X_va[:200]) #X_V is 200 already\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the fidelity of Lime and LioNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_expression = lambda s: re.split(r'\\W+', s)\n",
    "explainer = LimeTextExplainer(class_names=class_names, split_expression=split_expression)\n",
    "def lime_predict(text):\n",
    "    i = tk.texts_to_sequences(text)\n",
    "    i = pad_sequences(i, maxlen=max_len)\n",
    "    a = model.predict(i)\n",
    "    b = 1 - a \n",
    "    return np.column_stack((b,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts(text):\n",
    "    sent = ''\n",
    "    for i in text:    \n",
    "        if i != 0:\n",
    "            sent = sent + tk.index_word[i] + ' '\n",
    "    sent = sent[:-1]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_lime(text):\n",
    "    explanation = explainer.explain_instance(text_instance=tts(text), classifier_fn=lime_predict)\n",
    "    local_pred = explanation.local_pred[0]\n",
    "    return local_pred #This is because lime interprets class with label 1\n",
    "def fi_lionets(text):\n",
    "    _, _, loc_res, _, _ = lionet.explain_instance(text,2500,transparent_model)\n",
    "    return loc_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluation(model.predict,None,None,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidelity = evaluator.fidelity(train, [fi_lime, fi_lionets], class_n=0)\n",
    "print('Train:')\n",
    "print('  Lime fidelity:', fidelity[0][0])\n",
    "print('  LioNets fidelity:', fidelity[1][0])\n",
    "fidelity = evaluator.fidelity(valid, [fi_lime, fi_lionets], class_n=0)\n",
    "print('Valid:')\n",
    "print('  Lime fidelity:', fidelity[0][0])\n",
    "print('  LioNets fidelity:', fidelity[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the non zero weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = iutils.to_list(model.outputs)\n",
    "softmax_found = False\n",
    "ret = []\n",
    "for x in Xs:\n",
    "    layer, node_index, tensor_index = x._keras_history\n",
    "    if checks.contains_activation(layer, activation=\"sigmoid\"):\n",
    "        softmax_found = True\n",
    "        if isinstance(layer, keras.layers.Activation):\n",
    "            ret.append(layer.get_input_at(node_index))\n",
    "        else:\n",
    "            layer_wo_act = innvestigate.utils.keras.graph.copy_layer_wo_activation(layer)\n",
    "            ret.append(layer_wo_act(layer.get_input_at(node_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(input=model.input, output=ret)\n",
    "model2.trainable = False\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "analyzer = innvestigate.create_analyzer('lrp.epsilon', model2, neuron_selection_mode='max_activation', **{'epsilon': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_LRP(X_t):\n",
    "    ooo = analyzer.analyze(np.array([X_t]))[0]\n",
    "    ooo = ooo*np.array([0 if i == 0 else 1 for i in X_t]) #only on lrp\n",
    "    return [ooo][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_lime(text):\n",
    "    sent=tts(text)\n",
    "    explanation = explainer.explain_instance(text_instance=sent, classifier_fn=lime_predict)\n",
    "    weights = OrderedDict(explanation.as_list())\n",
    "    lime_w = {}\n",
    "    for k,v in weights.items():\n",
    "        lime_w[tk.word_index[k]] = v\n",
    "    interpretation = []\n",
    "    for i in text:\n",
    "        if i == 0:\n",
    "            interpretation.append(0)\n",
    "        else:\n",
    "            if i in lime_w.keys():\n",
    "                interpretation.append(lime_w[i])\n",
    "            else:\n",
    "                interpretation.append(0)\n",
    "    return np.array([interpretation])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_lionets(text):\n",
    "    weights, _, _, names, _ = lionet.explain_instance(text,2500,transparent_model)\n",
    "    lionets_w = {}\n",
    "    for v,k in dict(zip(list(weights[0]), list(names))).items():\n",
    "        if k == 'ukn':\n",
    "            lionets_w[tk.word_index['UKN']] = v\n",
    "        else:\n",
    "            lionets_w[tk.word_index[k]] = v\n",
    "    interpretation = []\n",
    "    for i in text:\n",
    "        if i == 0:\n",
    "            interpretation.append(0)\n",
    "        else:\n",
    "            interpretation.append(lionets_w[i])\n",
    "    return np.array([interpretation])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero = evaluator.non_zero_weights(train, [fi_LRP, fi_lime, fi_lionets])\n",
    "print('Train:')\n",
    "print('  LRP Non Zero:', non_zero[0][0])\n",
    "print('  Lime Non Zero:', non_zero[1][0])\n",
    "print('  LioNets Non Zero:', non_zero[2][0])\n",
    "non_zero = evaluator.non_zero_weights(valid, [fi_LRP, fi_lime, fi_lionets])\n",
    "print('Valid:')\n",
    "print('  LRP Non Zero:', non_zero[0][0])\n",
    "print('  Lime Non Zero:', non_zero[1][0])\n",
    "print('  LioNets Non Zero:', non_zero[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness = evaluator.robustness_embeddings(train,[fi_lime, fi_LRP, fi_lionets])\n",
    "print('Train:')\n",
    "print('  Lime Robustness:', robustness[0])\n",
    "print('  LRP Robustness:', robustness[1])\n",
    "print('  LioNets Robustness:', robustness[2])\n",
    "robustness = evaluator.robustness_embeddings(valid,[fi_lime, fi_LRP, fi_lionets])\n",
    "print('Valid:')\n",
    "print('  Lime Robustness:', robustness[0])\n",
    "print('  LRP Robustness:', robustness[1])\n",
    "print('  LioNets Robustness:', robustness[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altruist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for i in range(50):\n",
    "    features.append(str('f'+str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_LRP(X_t,prediction,model):\n",
    "    ooo = analyzer.analyze(np.array([X_t]))[0]\n",
    "    ooo = ooo*np.array([0 if i == 0 else 1 for i in X_t]) #only on lrp\n",
    "    return [ooo][0]\n",
    "def fi_lime(text,prediction,model):\n",
    "    sent=tts(text)\n",
    "    explanation = explainer.explain_instance(text_instance=sent, classifier_fn=lime_predict)\n",
    "    weights = OrderedDict(explanation.as_list())\n",
    "    lime_w = {}\n",
    "    for k,v in weights.items():\n",
    "        lime_w[tk.word_index[k]] = v\n",
    "    interpretation = []\n",
    "    for i in text:\n",
    "        if i == 0:\n",
    "            interpretation.append(0)\n",
    "        else:\n",
    "            if i in lime_w.keys():\n",
    "                interpretation.append(lime_w[i])\n",
    "            else:\n",
    "                interpretation.append(0)\n",
    "    return np.array([interpretation])[0]\n",
    "def fi_lionets(text,prediction,model):\n",
    "    weights, _, _, names, _ = lionet.explain_instance(text,2500,transparent_model)\n",
    "    lionets_w = {}\n",
    "    for v,k in dict(zip(list(weights[0]), list(names))).items():\n",
    "        if k == 'ukn':\n",
    "            lionets_w[tk.word_index['UKN']] = v\n",
    "        else:\n",
    "            lionets_w[tk.word_index[k]] = v\n",
    "    interpretation = []\n",
    "    for i in text:\n",
    "        if i == 0:\n",
    "            interpretation.append(0)\n",
    "        else:\n",
    "            interpretation.append(lionets_w[i])\n",
    "    return np.array([interpretation])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*Please let it run, it will take time probably*\")\n",
    "fi_names = {fi_LRP:'LRP',fi_lime:'Lime',fi_lionets:'LioNets'}\n",
    "fis = [fi_LRP, fi_lime,fi_lionets]\n",
    "fis_scores = []\n",
    "for i in fis:\n",
    "    fis_scores.append([])\n",
    "count = 0\n",
    "\n",
    "altruistino = Altruist(model, train, fis, features, None, True, None, True)\n",
    "for instance in train:            \n",
    "    if (count + 1) % 25 == 0:\n",
    "        print(count+1,\"/\",len(valid),\"..\",end=\", \")\n",
    "    #print(len(instance))\n",
    "    count = count + 1\n",
    "    untruthful_features = altruistino.find_untruthful_features(instance)\n",
    "    for i in range(len(untruthful_features[0])):\n",
    "        fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "count = 0\n",
    "print()\n",
    "print('Train:')\n",
    "for fis_score in fis_scores:\n",
    "    fi = fis[count]\n",
    "    count = count + 1\n",
    "    print(' ',fi_names[fi],np.array(fis_score).mean())\n",
    "fi_matrix = np.array(fis_scores)\n",
    "count = 0\n",
    "fi_all = []\n",
    "for instance in train:\n",
    "    fi_all.append(fi_matrix[:,count].min())\n",
    "    count = count + 1\n",
    "print(\"Altogether:\",np.array(fi_all).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*Please let it run, it will take time probably*\")\n",
    "fi_names = {fi_LRP:'LRP',fi_lime:'Lime',fi_lionets:'LioNets'}\n",
    "fis = [fi_LRP, fi_lime,fi_lionets]\n",
    "fis_scores = []\n",
    "for i in fis:\n",
    "    fis_scores.append([])\n",
    "count = 0\n",
    "\n",
    "altruistino = Altruist(model, train, fis, features, None, True, None, True)\n",
    "for instance in valid[:2]:            \n",
    "    if (count + 1) % 25 == 0:\n",
    "        print(count+1,\"/\",len(valid),\"..\",end=\", \")\n",
    "    #print(len(instance))\n",
    "    count = count + 1\n",
    "    untruthful_features = altruistino.find_untruthful_features(instance)\n",
    "    for i in range(len(untruthful_features[0])):\n",
    "        fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "count = 0\n",
    "print()\n",
    "print('Valid:')\n",
    "for fis_score in fis_scores:\n",
    "    fi = fis[count]\n",
    "    count = count + 1\n",
    "    print(' ',fi_names[fi],np.array(fis_score).mean())\n",
    "fi_matrix = np.array(fis_scores)\n",
    "count = 0\n",
    "fi_all = []\n",
    "for instance in train:\n",
    "    fi_all.append(fi_matrix[:,count].min())\n",
    "    count = count + 1\n",
    "print(\"Altogether:\",np.array(fi_all).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test an instance and its explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_T[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, a, b, names, c = lionet.explain_instance(X_tr[22],2500,transparent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lionets_w = {}\n",
    "for v,k in dict(zip(list(weights[0]), list(names))).items():\n",
    "    if k == 'ukn':\n",
    "        lionets_w[tk.word_index['UKN']] = v\n",
    "    else:\n",
    "        lionets_w[tk.word_index[k]] = v\n",
    "interpretation = []\n",
    "for i in X_tr[22]:\n",
    "    if i != 0:\n",
    "        interpretation.append([tk.index_word[i],lionets_w[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(num=None, figsize=(4, 3), dpi=200, facecolor='w', edgecolor='k')\n",
    "i_weights = pd.DataFrame({\"Features\": [o[0] for o in np.array(interpretation)[:,:1]], \n",
    "                                  \"Features' Weights\": [float(o[0]) for o in np.array(interpretation)[:,1:]]})\n",
    "i_weights = i_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "i_weights = i_weights.drop_duplicates()\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Features\", data=i_weights)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_predict(['or maybe just do not follow UKN UKN from the UKN UKN'])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(10, 8), dpi=250, facecolor='w', edgecolor='k')\n",
    "i_weights = pd.DataFrame({\"Features\": names, \n",
    "                                  \"Features' Weights\":weights[0]})\n",
    "i_weights = i_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "i_weights = i_weights.drop_duplicates()\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Features\", data=i_weights)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_weights = []\n",
    "counter_features = []\n",
    "for i in range(len(weights[0])):\n",
    "    if weights[0][i]!=0:\n",
    "        if names[i] not in X_T[22].lower():\n",
    "            counter_weights.append(weights[0][i])\n",
    "            counter_features.append(names[i])\n",
    "co_weights = pd.DataFrame({\"Counter Features\": list(counter_features), \n",
    "                                  \"Features' Weights\": list(counter_weights)})\n",
    "co_weights = co_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "co_weights = pd.concat([co_weights.head(3),co_weights.tail(3)])\n",
    "plt.figure(num=None, figsize=(4, 3), dpi=200, facecolor='w', edgecolor='k')\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Counter Features\", data=co_weights)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.title(str('Features not appearing in the instance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_predict(['or maybe just do not follow me UKN religions from the UKN UKN'])[0][1]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "setC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
